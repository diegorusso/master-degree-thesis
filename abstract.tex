\begin{abstract}
In the last decade Deep Learning has had a incredible success due to the
increasing availability of big data and the decreasing cost of compute power.
The research has been massive in all fields around Deep Learning: from new
network models to solve different problems to optimizations of such models
for deployment to edge devices with limited resources.
With almost 100 papers in the literature, pruning is amongst these techniques
with the aim to optimize the models to be more power and memory efficient while
having no or little drop in the accuracy.
This thesis shows a specific pruning technique: layers of a network model are
pruned based on a heuristic whilst respecting the target sparsity of the
network model.
After explaining the theory behind this approach, the code is explained and
applied to a simple MNIST based network model using TensorFlow Model
Optimization.
The ending chapter shows results of experiments running on MobileNet v1
architecture using CIFAR-10 and ImageNet 2012 as datasets.
The heuristic distribution of weights behave more robustly compared to the
uniform distribution especially with higher sparsity levels.
\end{abstract}