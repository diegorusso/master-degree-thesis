\chapter{Pruning}\label{ch:pruning}
In \autoref{ch:introduction} I gave a brief explanation of few techniques for
doing model optimisation on a neural network. Pruning is one of these and in
the first part of this chapter I give a more detailed explanation.
The second part instead focuses on the core of the thesis: per-layer pruning
configuration with heuristic.

Despite being technical, this chapter is still fairly theoretical and I defer
any code and implementation to \autoref{ch:implementation}.

\section{What's Pruning?}
Neural network pruning is the task of reducing the size of a network by
removing parameters. This compression affects the size of the model, the
latency, the amount of memory and the compute power needed to run the
inference. These metrics need to balanced with the accuracy of the model
itself. I give a more detailed analysis about this trade-off in
\autoref{subsec:tradeoff}

Pruning has been used since the late 1980s but has seen an explosion of
interest in the past decade thanks to the rise of deep neural networks.
It sets its roots with a couple of classic papers:\textit{Optimal Brain
Damage}~\cite{lecun-90b} and \textit{Optimal Brain Surgeon}\cite{hassibi-93}

In the last decade (2010\-2020) a few dozens of papers have been published in
literature about pruning and all of them have been showing that pruning is an
effective technique that can be applied to a variety of neural network on
different fields (image and speech recognition, text processing, etc\ldots).
Moreover they highlights that pruning is a versatile technique as, I said
earlier, it has a positive impact on multiple metrics, all important for a
better edge deployment of the model.

How does pruning reduce the size of a model? The basic principle is to prune
(remove) unnecessary neurones or weights (see \autoref{fig:pruning_weights_neurons}):

\begin{figure}[ht]
    \includegraphics[width=8cm]{images/pruning/pruning_weights_neurons.png}
    \centering
    \caption{Pruning weights and neurons}\label{fig:pruning_weights_neurons}
\end{figure}

\begin{itemize}
    \item \textbf{weights}: this is done by setting individual parameters to
        zero and making the network sparse. The effect will be to maintain the
        same architecture of the network but lowering down the number of
        parameters.
    \item \textbf{neurons}: this is done by removing the entire node from the
        network with all its connections. This would make the network
        architecture smaller but with the target to keep the accuracy of the
        starting network.
\end{itemize}

\subsection{Pruning techniques}
The main problem in pruning is to understand what to prune. Of course the goal
is to remove nodes and/or weights that are less useful. There are different
methods to understand what to prune with very little or no effect on accuracy.
Below e brief description of different techniques.

\subsubsection{Magnitude Pruning}
Functions can be a very simple case of a neural network. Its coefficients can
be changed in order to learn the input data points. There are coefficients
that, despite changing their values, they won't change the behaviour of the
function and these are referred as \textbf{non-significant}.
In neural networks these coefficients are weights and biases: they are
\textbf{trainable parameters} and the same non-significant concept can be
applied to them with bit more complexity.

During the back-propagation (gradient descent) some weights are updated with
larger gradient magnitudes (both positive and negative) than the others.
These weights are the \textbf{significant} ones and the weights receiving very
small gradients can be considered as \textbf{non-significant} as their impact
is minimal to the optimization of the loss function.
After the training, the weight magnitude of every layer can be explored and
check which weights are significant.

So the weight magnitude is the criteria for pruning the neural network.
At this point a \textbf{threshold} is specified and all the weights below this
threshold are considered non-significant. This is usually combined with a
\textbf{sparsity target} the network should achieve.
The \textbf{non-significant weights will be zeroed}, cancelling effectively
their impact in the neural network.
This can be applied to biases as well (to any trainable parameter to be
precise).

Once the pruning is done it's always advisable to retrain the network in order
to compensate for any drop in performance. It's worth noticing that during the
retraining the pruned weights won't be updated.~\cite{magnitude_pruning}

Magnitude pruning is the technique I will be using in
\autoref{ch:implementation}.

\subsubsection{Channel Pruning}
Channel pruning is a technique specifically for CNN (Convolutional Neural
Network) as it rely on the architecture of this type of networks. The building
blocks of a CNN are (see \autoref{fig:cnn}):

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/pruning/cnn.jpeg}
    \centering
    \caption{Convolutional Neural Network Architecture}\label{fig:cnn}
\end{figure}

\begin{itemize}
    \item \textbf{Convolutional layer}: it is the core building block of a CNN\@.
        A convolution is the simple application of a filter to an input that
        results in an activation. Repeated application of the same filter to an
        input results in a map of activations called a feature map, indicating
        the locations and strength of a detected feature in an input, such as
        an image.
    \item \textbf{Pooling layer}: it provide an approach to down sampling
        feature maps by summarizing the presence of features in patches of the
        feature map. Two common pooling methods are average pooling and max
        pooling that summarize the average presence of a feature and the most
        activated presence of a feature respectively.
    \item \textbf{ReLU layer}: ReLU stands for \textbf{Rectified Linear Unit}
        and it is a piecewise linear function that will output the input
        directly if it is positive, otherwise, it will output zero. It has
        become the default activation function for many types of neural
        networks because a model that uses it is easier to train and often
        achieves better performance (see \autoref{fig:relu})
\begin{figure}[ht]
    \includegraphics[width=8cm]{images/pruning/relu.png}
    \centering
    \caption{Rectified Linear Unit activation function}\label{fig:relu}
\end{figure}
    \item \textbf{Fully connected layer}: after several convolutional and max
        pooling layers, the high-level reasoning in the neural network is done
        via fully connected layers. Neurons in a fully connected layer have
        connections to all activations in the previous layer, as seen in
        non-convolutional artificial neural networks.
    \item \textbf{Loss layer}: it specifies how training penalizes the
        deviation between the predicted (output) and true labels and is
        normally the final layer of a neural network. Various loss functions
        appropriate for different tasks may be used. Softmax loss is used for
        predicting a single class of K mutually exclusive classes.
        Sigmoid cross-entropy loss is used for predicting K independent
        probability values in [0, 1]. Euclidean loss is used for regressing to
        real-valued labels.\cite{cnn}
\end{itemize}

\autoref{fig:channel_pruning} shows the channel pruning algorithm for a
single convolutional layer.
The aim is to reduce the number of channels of feature map B, while maintaining
outputs in feature map C.
Once the channels are pruned, corresponding channels of the filters that take
these channels as input can be removed. Moreover, filters that produce these
channels can be removed as well. It is clear that channel pruning involves
two key steps.

\begin{figure}[ht]
    \includegraphics[width=8cm]{images/pruning/channel_pruning.png}
    \centering
    \caption{Channel Pruning for accelerating a CNN}\label{fig:channel_pruning}
\end{figure}

The first is the channel selection, since a proper channel combination to
maintain as much information needs to be selected.
The second is reconstruction. The following feature maps need to be
reconstructed using the selected channels. Motivated by this, the process is an
iterative two-step algorithm.

In the first step, the aim is to select most representative channels.
Since an exhaustive search is infeasible even for tiny networks, a LASSO
regression based method needs to be performed to figure out representative
channels and prune redundant ones.

In the second step, outputs are reconstructed with remaining channels with
linear least squares.

The whole model can be pruned applying the approach layer by layer
sequentially. For each layer, input volumes are obtained from the current input
feature map, and output volumes from the output feature map of the un-pruned
model.\cite{He_2017}

\subsubsection{Structured Pruning}
Pruning techniques can be broadly categorized as structured or unstructured.
Unstructured pruning does not follow a specific geometry or constraint. In most
cases, this technique needs extra information to represent sparse locations. It
depends on sparse representation for computational benefits. On the other hand,
structured sparsity places non-zero parameters at well-defined locations. This
kind of constraint enables modern CPUs and graphics processing units (GPUs) to
easily exploit computational savings.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/pruning/structured_pruning.png}
    \centering
    \caption{Structured Pruning}\label{fig:structured_pruning}
\end{figure}

Channel, kernel and intra-kernel sparsity could be a mean of structured
pruning. In channel level pruning, all the incoming and outgoing weights
to/from a feature map are pruned. Channel level pruning can directly produce a
lightweight network.
Kernel level pruning drops a full k × k kernel, whereas the intra-kernel
sparsity prunes weights in a kernel. The intra kernel strided sparsity can
significantly speed-up convolution layer processing.
The kernel level pruning is a special case of intra-kernel sparsity with
100\% pruning. These granularities can be applied in various combinations and
different orders.\cite{Anwar_2017}

\subsection{Pruning pipeline}
So far I've been explaining what pruning is and what kind of techniques exist
to pruning neural networks. Let's take a step back and see how these techniques
fit the big picture of the pruning pipeline.
There are different ways to apply pruning to neural networks:

\subsubsection{Traditional network pruning pipeline}

This is the traditional approach for pruning a neural network. It prunes
redundant connections using a three-step method:
\begin{enumerate}
    \item Train the network to learn which connections are important: unlike
        conventional training, the final values of the weights are not learnt,
        but rather which connections are important.
    \item Prune the unimportant connections: all connections with weights below
        a threshold are removed from the network, converting a dense network
        into a sparse network.
    \item Retrain the network to fine tune the weights of the remaining
        connections: that's the critical step because if the pruned network is
        used without retraining, accuracy is significantly impacted.
\end{enumerate}

Learning the right connections is an \textbf{iterative process}. Pruning
followed by a retraining is one iteration, after many such iterations the
minimum number connections could be found.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/pruning/traditional_pipeline.png}
    \centering
    \caption{Traditional pruning pipeline}\label{fig:traditional_pipeline}
\end{figure}

After pruning connections, neurons with zero input connections or zero output
connections may be safely pruned. This pruning is furthered by removing all
connections to or from a pruned neuron. The retraining phase automatically
arrives at the result where dead neurons will have both zero input connections
and zero output connections.
This occurs due to gradient descent and regularization. A neuron that has zero
input connections (or zero output connections) will have no contribution to the
final loss, leading the gradient to be zero for its output connection (or input
connection), respectively.
Only the regularization term will push the weights to zero. Thus, the dead
neurons will be automatically removed during retraining.~\cite{han2015learning}

In this thesis I focus mainly on the traditional pipeline but the work can be
easily used with other pipelines.

\subsubsection{Training from scratch}
Another pipeline is ``training from scratch'' where the network pruning is
rethought.
Generally, there are two common beliefs behind this pruning procedure. First,
it is believed that starting with training a large, over-parametrized network
is important as it provides a high-performance model (due to stronger
representation \& optimization power) from which one can safely remove a set of
redundant parameters without significantly hurting the accuracy. Therefore,
this is usually believed, and reported to be superior to directly training a
smaller network from scratch \- a commonly used baseline approach.
Second, both the pruned architecture and its associated weights are believed to
be essential for obtaining the final efficient model. Thus most existing
pruning techniques choose to fine-tune a pruned model instead of training it
from scratch. The preserved weights after pruning are usually considered to be
critical.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/pruning/liu_pipeline.png}
    \centering
    \caption{Pruning pipeline with training from scratch}\label{fig:liu_pipeline}
\end{figure}

With training from scratch, both of the beliefs mentioned above are not
necessarily true for structured pruning methods, which prune at the levels of
convolution channels or larger.
For structured pruning methods with predefined target network architectures,
directly training the small target model from random initialization can achieve
the same, if not better, performance, as the model obtained from the
three-stage pipeline.
For structured pruning methods with auto-discovered target networks, training
the pruned model from scratch can also achieve comparable or even better
performance than fine-tuning.
Interestingly, for unstructured pruning method that prunes individual
parameters, training from scratch can mostly achieve comparable accuracy with
pruning and fine-tuning on smaller-scale datasets, but fails to do so on the
large-scale ImageNet benchmark.
Note that in some cases, if a pre-trained large model is already available,
pruning and fine-tuning from it can save the training time required to obtain
the efficient model.~\cite{liu2018rethinking}

\subsubsection{Pruning from scratch}
In this pipeline, pre-training an over-parametrized model is not necessary
step for obtaining the target pruned structure. In fact, a fully-trained
over-parametrized model will reduce the search space for the pruned structure.

Is it necessary for learning the pruned model structure from pre-trained
weights?
After some research it has been found that the answer is quite surprising: an
effective pruned structure does not have to be learned from pre-trained
weights.

It has been empirically shown that the pruned structures discovered from
pre-trained weights tend to be homogeneous, which limits the possibility of
searching for better structure.

In  fact, more diverse and effective pruned structures can be discovered by
directly pruning from randomly initialized weights, including  potential models
with better performance. Based  on  the  above  observations, it has been
created a novel network pruning pipeline where a pruned network structure can
be directly learned from the randomly initialized weights (see
\autoref{fig:scratch_pipeline}). This pruning pipeline allows \textbf{pruning
from scratch.}

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/pruning/scratch_pipeline.png}
    \centering
    \caption{Pruning pipeline with pruning from scratch}\label{fig:scratch_pipeline}
\end{figure}

Specifically, it is used a similar technique in Network Slimming to learn the
channel importance by associating scalar gate values with each layer.
The channel importance is optimized to improve the model performance under the
sparsity regularization.
What is different from previous works is that random weights are not updated
during this process. After finishing the learning of channel importance, a
simple binary search strategy is used to determine the channel number
configurations of the pruned model given resource constraints (e.g., FLOPS).
Since it is not needed to update the model weights during optimization, the
pruned structure can be discovered at an extremely fast speed.

This approach not only greatly reduces the pre-training burden of traditional
pruning methods, but also achieves similar or even higher accuracy under the
same computation budget.~\cite{Wang_2020}

\subsection{Sparsity-accuracy trade-offs}\label{subsec:tradeoff}
\lipsum[1]

\subsection{Practical examples with Pruning}
\lipsum[1]

\section{Per-Layer Pruning Configuration With Heuristic}\label{sec:heuristic}
\lipsum[1]

\subsection{What's a heuristic?}
\lipsum[1]

\subsection{Heuristic details}
Explain in details the technique and the reasons behind
\lipsum[1]

\subsection{Custom heuristic formula}
\lipsum[1]
