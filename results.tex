\chapter{Results}\label{ch:results}
In this chapter I show how the pruning with heuristic affects MobileNet
v1\cite{howard2017mobilenets} based models. The experiments will be run using
both CIFAR-10\cite{cifar_10} and ImageNet 2012\cite{imagenet_cvpr09}.
Before showing to the results, I'll give an overview about MobileNet v1
architecture and the respective datasets used in the experiments.

\section{MobileNet v1}
MobileNet is a class of efficient models for mobile and embedded vision
applications (\autoref{fig:mobilenet_applications}) developed by Google.
It is based on a streamlined architecture that uses depthwise separable
convolutions to build light weight deep neural networks.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/results/mobilenet_applications.png}
    \centering
    \caption{MobileNet applications}\label{fig:mobilenet_applications}
\end{figure}

Convolutional neural networks have been more popular in the recent years and
their accuracy increased thanks to more complex architecture increasing their
size whilst impacting negatively in speed. In many real world applications,
these networks need to run on edge devices with limited resources and the
inferences need to be carried out in a timely fashion.

The main focus of MobileNet is to increase the efficiency of the network by
decreasing the number of parameters by not compromising
performance\cite{review_mobilenet}.

\subsection{Depthwise Separable Convolution}
Depthwise separable convolution is the core basis of MobileNet architecture. It
is a \textbf{depthwise convolution followed by a pointwise convolution}.

A normal convolution is shown in \autoref{fig:convolution}

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/results/convolution.png}
    \centering
    \caption{Convolution}\label{fig:convolution}
\end{figure}

In the figure there is an input image size of $12\times12\times3$ and the
kernel (or filter) is $5\times5\times3$ with stride = 1. There are though 10
kernels to apply and this gives an output image of $8\times8\times10$.
The total computational cost is \bm{$12\times12\times5\times5\times3\times10 =
108000$}.

There are the following dimensions:
\begin{itemize}
    \item Input image: \bm{$D_f \times D_f \times M$}
    \item Output image: \bm{$D_f \times D_f \times N$}
    \item Convolution kernel: \bm{$D_k \times D_k \times M \times N$}
\end{itemize}

So in a normal convolution the total computational cost is
\bm{$D_k \times D_k \times M \times N \times D_f \times D_f$}

The above convolution can be split in 2 phases:
\begin{enumerate}
    \item Depthwise convolution
    \item Pointwise convolution
\end{enumerate}

The first one is the \textbf{depthwise convolution} and it is shown in
\autoref{fig:depthwise_convolution}.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/results/depthwise_convolution.png}
    \centering
    \caption{Depthwise convolution}\label{fig:depthwise_convolution}
\end{figure}

In this case the input has 3 channels and there are 3 $5\times5\times1$
kernels. These 3 kernels are applied to the three channels respectively
producing 3 $8\times8\times1$ output. When the 3 outputs are stacked the final
output is $8\times8\times3$.

The second phase is the \textbf{pointwise convolution} as shown in
\autoref{fig:pointwise_convolution}.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/results/pointwise_convolution.png}
    \centering
    \caption{Pointwise convolution}\label{fig:pointwise_convolution}
\end{figure}

The output image of the previous step is the input image for this step.
The convolution is done using a $1\times1\times3$ kernel on the input image
producing a feature map. Repeating this using 10 different $1\times1\times3$
kernels will produce 10 different feature maps that will be stacked together.

The computation for each step is:
\begin{enumerate}
    \item Depthwise convolution: \bm{$12 \times 12 \times 5 \times 5 \times 3 =
        10800$}
    \item Pointwise convolution: \bm{$8 \times 8 \times 3 \times 10 = 1920$}
\end{enumerate}

Therefore the total number of computations is \textbf{10800 + 1920 = 12720}

More generically the computational cost is \bm{$D_k \times D_k \times M \times
D_f \times D_f + M \times N \times D_f \times D_f$}

In this specific case using a kernel of $3 \times 3$ there is about 8 to 9
times less computational reduction: \bm{$108000 / 12720 \approx 8.45$}

\subsection{MobileNet architecture}
The network architecture is built on depthwise separable convolutions except
for the first layer which is a full convolution

There are 28 convolutional layers (counting depthwise and pointwise layers) and
1 fully connected layer followed by a softmax layer
(\autoref{fig:mobilenet_architecture}).

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/results/mobilenet_architecture.png}
    \centering
    \caption{MobileNet architecture}\label{fig:mobilenet_architecture}
\end{figure}

All layers are followed by a batch normalization and ReLU non linearity
(\autoref{fig:mobilenet_convolution}) with the exception of the final fully
connected layer which has no non linearity and feeds into a softmax layer for
classification.
As seen earlier in the thesis, the softmax is used to to predict a single class
of K mutually exclusive classes. In the case of MobileNet, when used with
ImageNet, it can classify up to 1000 classes.

\begin{figure}[ht]
    \includegraphics[width=8cm]{images/results/mobilenet_convolution.png}
    \centering
    \caption{Normal convolution vs MobileNet convolution}\label{fig:mobilenet_convolution}
\end{figure}

\subsection{Width Multiplier}
MobileNet has been developed to be small and low latency but sometimes specific
use cases or applications need the model to be faster and smaller.
In order to construct these smaller and less computationally expensive models
a very simple parameter \bm{$\alpha$} called width multiplier has been
introduced.

The role of the width multiplier $\alpha$ is to thin a network uniformly at
each layer: the number of input channels M becomes $\alpha M$ and the number of
output channels N becomes $\alpha N$.

So depth wise separable computational cost becomes \bm{$D_k \times D_k \times
\alpha M \times D_f \times D_f + \alpha M \times \alpha N \times D_f \times
D_f$} where $\alpha \in \interval[open left]{0}{1}$ with typical settings of 1,
0.75, 0.5, 0.25.

The \autoref{fig:mobilenet_widthmultiplier} shows the impact that $\alpha$ has
on accuracy, numbers of operations and parameters.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/results/mobilenet_widthmultiplier.png}
    \centering
    \caption{Width multiplier impact}\label{fig:mobilenet_widthmultiplier}
\end{figure}

In this thesis I consider \bm{$\alpha = 1$} to be the baseline.

\subsection{Resolution Multiplier}
The second hyper-parameter to reduce the computational cost of a neural network
is a resolution multiplier \bm{$\rho$}. This can be applied to the input image
and the internal representation of every layer is subsequently reduced by the
same multiplier.
Including $\rho$, the computational cost becomes \bm{$D_k \times D_k \times
\alpha M \times \rho D_f \times \rho D_f + \alpha M \times \alpha N \times \rho
D_f \times \rho D_f$} where $\rho \in \interval[open left]{0}{1}$i which is
typically set implicitly so that the input resolution of the network is 224,
192, 160 or 128.

The \autoref{fig:mobilenet_resolutionmultiplier} shows the impact that $\rho$
has on accuracy, numbers of operations and parameters.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/results/mobilenet_resolutionmultiplier.png}
    \centering
    \caption{Resolution multiplier impact}\label{fig:mobilenet_resolutionmultiplier}
\end{figure}

In this thesis I consider \bm{$\rho = 1$} to be the baseline.

\section{Datasets: CIFAR-10 and ImageNet}
In this section, I give a brief explanation of CIFAR-10 and ImageNet datasets.

\subsection{CIFAR-10 dataset}
The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection
of images that are commonly used to train neural networks. It is one of the
most widely used datasets for machine learning research.

It consists of \textbf{60000} \bm{$32 \times 32$} colour images in
\textbf{10 classes}, with 6000 images per class. There are 50000 training
images and 10000 test images.

The dataset is divided into five training batches and one test batch, each with
10000 images. The test batch contains exactly 1000 randomly-selected images
from each class. The training batches contain the remaining images in random
order, but some training batches may contain more images from one class than
another. Between them, the training batches contain exactly 5000 images from
each class.

The 10 different classes represent airplanes, cars, birds, cats, deer, dogs,
frogs, horses, ships, and trucks (\autoref{fig:CIFAR_10}).

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/results/CIFAR_10.png}
    \centering
    \caption{CIFAR-10 sample images}\label{fig:CIFAR_10}
\end{figure}

The classes are completely mutually exclusive. There is no overlap between
automobiles and trucks. ``Automobile'' includes sedans, SUVs, things of that
sort. ``Truck'' includes only big trucks. Neither includes pick-up trucks.

\subsection{ImageNet 2012 dataset}
ImageNet is an image dataset organized according to the WordNet hierarchy. Each
meaningful concept in WordNet, possibly described by multiple words or word
phrases, is called a ``synonym set'' or ``synset''. There are more than 100000
synsets in WordNet, majority of them are nouns (80000+). In ImageNet, there are
on average 1000 images to illustrate each synset.
Images of each concept are quality-controlled and human-annotated. In its
completion, ImageNet will offer tens of millions of cleanly sorted images for
most of the concepts in the WordNet hierarchy (\autoref{fig:imagenet})

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/results/imagenet.png}
    \centering
    \caption{ImageNet sample images}\label{fig:imagenet}
\end{figure}

The dataset I've used for the experiments has:

\begin{itemize}
    \item Train split: \textbf{1281167 images}
    \item Validation split: \textbf{50000 images}
\end{itemize}

The ILSVRC uses a ``trimmed'' list of only \textbf{1000 image categories} or
``classes'', including 90 of the 120 dog breeds classified by the full ImageNet
schema.

\section{Experiment results}
After a brief explanation of the model and the datasets used for the
experiments, in this section I show what results I have in pruning MobileNet v1
with CIFAR-10 and ImageNet.

\subsection{Environments}
The environment used for running CIFAR-10 experiments has the following
characteristics:

\begin{itemize}
    \item \textbf{CPU and memory:} 2 x Intel Xeon Gold 5120T CPU @ 2.20GHz, 28
        cores (56 threads) total, 64GB memory
    \item \textbf{Graphic:} NVIDIA TITAN Xp, 12GB GDDR5X, 3840 cores, CUDA
        driver version: 460.27.04, CUDA version: 11.2
    \item \textbf{Operating System:} Ubuntu 18.04.5 LTS, kernel
        5.4.0\-60-generic
    \item \textbf{Software Stack:} Python 3.8.5, TensorFlow 2.4.0, TFMOT
        0.5.0.dev20210206 with my patch for heuristic distribution, and
        dependencies. Everything has been isolated using conda
        (\url{https://docs.conda.io/en/latest/})
\end{itemize}

The environment used for running ImageNet experiments has the following
characteristics:

\begin{itemize}
    \item \textbf{CPU and memory:} AWS p3.2xlarge, 8 vCPU, 61GB memory
    \item \textbf{Graphic:} NVIDIA Tesla V100, 16GB, 5120 cores, CUDA Driver
        Version: 450.80.02, CUDA Version: 11.0
    \item \textbf{Operating System:} Ubuntu 18.04.5 LTS, kernel
        5.4.0\-1037-aws
    \item \textbf{Software Stack:} Python 3.7.6, TensorFlow 2.4.1, TFMOT
        0.5.0.dev20210206 with my patch for heuristic distribution, and
        dependencies. Everything has been isolated using conda
        (\url{https://docs.conda.io/en/latest/})
\end{itemize}

The reason I chose AWS for ImageNet is because of the time the training takes
with this dataset: one epoch is about 1.5h and I needed to run 20 epochs.
As I show later, the number of experiments are 20 and AWS gives me the
flexibility to have multiple instances in parallel.

\subsection{MobileNet v1 with CIFAR-10}
Mention the stride has been modified to 1.
Mention the baseline accuracy and size of the model.

Experiment results: \todo{Experiment results with CIFAR-10}

\subsection{MobileNet v1 with ImageNet 2012}
After seen the results using CIFAR-10, in this section the dataset used is
ILSVRC (ImageNet Large Scale Visual Recognition Challenge) 2012, commonly known
as ImageNet.

Experiment results: \todo{Experiment results with ImageNet}
