\chapter{Results}\label{ch:results}
In this chapter I show how the pruning with heuristic affects MobileNet
v1\cite{howard2017mobilenets} based models. The experiments will be run using
both CIFAR-10\cite{cifar_10} and ImageNet 2012\cite{imagenet_cvpr09}.
Before showing to the results, I'll give an overview about MobileNet v1
architecture and the respective datasets used in the experiments.

\section{MobileNet v1}
MobileNet is a class of efficient models for mobile and embedded vision
applications (\autoref{fig:mobilenet_applications}) developed by Google.
It is based on a streamlined architecture that uses depthwise separable
convolutions to build light weight deep neural networks.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/results/mobilenet_applications.png}
    \centering
    \caption{MobileNet applications}\label{fig:mobilenet_applications}
\end{figure}

Convolutional neural networks have been more popular in the recent years and
their accuracy increased thanks to more complex architecture increasing their
size whilst impacting negatively in speed. In many real world applications,
these networks need to run on edge devices with limited resources and the
inferences need to be carried out in a timely fashion.

The main focus of MobileNet is to increase the efficiency of the network by
decreasing the number of parameters by not compromising
performance\cite{review_mobilenet}.

\subsection{Depthwise Separable Convolution}
Depthwise separable convolution is the core basis of MobileNet architecture. It
is a \textbf{depthwise convolution followed by a pointwise convolution}.

A normal convolution is shown in \autoref{fig:convolution}

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/results/convolution.png}
    \centering
    \caption{Convolution}\label{fig:convolution}
\end{figure}

In the figure there is an input image size of $12\times12\times3$ and the
kernel (or filter) is $5\times5\times3$ with stride = 1. There are though 10
kernels to apply and this gives an output image of $8\times8\times10$.
The total computational cost is \bm{$12\times12\times5\times5\times3\times10 =
108000$}.

There are the following dimensions:
\begin{itemize}
    \item Input image: \bm{$D_f \times D_f \times M$}
    \item Output image: \bm{$D_f \times D_f \times N$}
    \item Convolution kernel: \bm{$D_k \times D_k \times M \times N$}
\end{itemize}

So in a normal convolution the total computational cost is
\bm{$D_k \times D_k \times M \times N \times D_f \times D_f$}

The above convolution can be split in 2 phases:
\begin{enumerate}
    \item Depthwise convolution
    \item Pointwise convolution
\end{enumerate}

The first one is the \textbf{depthwise convolution} and it is shown in
\autoref{fig:depthwise_convolution}.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/results/depthwise_convolution.png}
    \centering
    \caption{Depthwise convolution}\label{fig:depthwise_convolution}
\end{figure}

In this case the input has 3 channels and there are 3 $5\times5\times1$
kernels. These 3 kernels are applied to the three channels respectively
producing 3 $8\times8\times1$ output. When the 3 outputs are stacked the final
output is $8\times8\times3$.

The second phase is the \textbf{pointwise convolution} as shown in
\autoref{fig:pointwise_convolution}.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/results/pointwise_convolution.png}
    \centering
    \caption{Pointwise convolution}\label{fig:pointwise_convolution}
\end{figure}

The output image of the previous step is the input image for this step.
The convolution is done using a $1\times1\times3$ kernel on the input image
producing a feature map. Repeating this using 10 different $1\times1\times3$
kernels will produce 10 different feature maps that will be stacked together.

The computation for each step is:
\begin{enumerate}
    \item Depthwise convolution: \bm{$12 \times 12 \times 5 \times 5 \times 3 =
        10800$}
    \item Pointwise convolution: \bm{$8 \times 8 \times 3 \times 10 = 1920$}
\end{enumerate}

Therefore the total number of computations is \textbf{10800 + 1920 = 12720}

More generically the computational cost is \bm{$D_k \times D_k \times M \times
D_f \times D_f + M \times N \times D_f \times D_f$}

In this specific case using a kernel of $3 \times 3$ there is about 8 to 9
times less computational reduction: \bm{$108000 / 12720 \approx 8.45$}

\subsection{MobileNet architecture}
The network architecture is built on depthwise separable convolutions except
for the first layer which is a full convolution

There are 28 convolutional layers (counting depthwise and pointwise layers) and
1 fully connected layer followed by a softmax layer
(\autoref{fig:mobilenet_architecture}).

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/results/mobilenet_architecture.png}
    \centering
    \caption{MobileNet architecture}\label{fig:mobilenet_architecture}
\end{figure}

All layers are followed by a batch normalization and ReLU non linearity
(\autoref{fig:mobilenet_convolution}) with the exception of the final fully
connected layer which has no non linearity and feeds into a softmax layer for
classification.
As seen earlier in the thesis, the softmax is used to to predict a single class
of K mutually exclusive classes. In the case of MobileNet, when used with
ImageNet, it can classify up to 1000 classes.

\begin{figure}[ht]
    \includegraphics[width=8cm]{images/results/mobilenet_convolution.png}
    \centering
    \caption{Normal convolution vs MobileNet convolution}\label{fig:mobilenet_convolution}
\end{figure}

\subsection{Width Multiplier}
MobileNet has been developed to be small and low latency but sometimes specific
use cases or applications need the model to be faster and smaller.
In order to construct these smaller and less computationally expensive models
a very simple parameter \bm{$\alpha$} called width multiplier has been
introduced.

The role of the width multiplier $\alpha$ is to thin a network uniformly at
each layer: the number of input channels M becomes $\alpha M$ and the number of
output channels N becomes $\alpha N$.

So depth wise separable computational cost becomes \bm{$D_k \times D_k \times
\alpha M \times D_f \times D_f + \alpha M \times \alpha N \times D_f \times
D_f$} where $\alpha \in \interval[open left]{0}{1}$ with typical settings of 1,
0.75, 0.5, 0.25.

The \autoref{fig:mobilenet_widthmultiplier} shows the impact that $\alpha$ has
on accuracy, numbers of operations and parameters.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/results/mobilenet_widthmultiplier.png}
    \centering
    \caption{Width multiplier impact}\label{fig:mobilenet_widthmultiplier}
\end{figure}

In this thesis I consider \bm{$\alpha = 1$} to be the baseline.

\subsection{Resolution Multiplier}
The second hyper-parameter to reduce the computational cost of a neural network
is a resolution multiplier \bm{$\rho$}. This can be applied to the input image
and the internal representation of every layer is subsequently reduced by the
same multiplier.
Including $\rho$, the computational cost becomes \bm{$D_k \times D_k \times
\alpha M \times \rho D_f \times \rho D_f + \alpha M \times \alpha N \times \rho
D_f \times \rho D_f$} where $\rho \in \interval[open left]{0}{1}$i which is
typically set implicitly so that the input resolution of the network is 224,
192, 160 or 128.

The \autoref{fig:mobilenet_resolutionmultiplier} shows the impact that $\rho$
has on accuracy, numbers of operations and parameters.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/results/mobilenet_resolutionmultiplier.png}
    \centering
    \caption{Resolution multiplier impact}\label{fig:mobilenet_resolutionmultiplier}
\end{figure}

In this thesis I consider \bm{$\rho = 1$} to be the baseline.

\section{Datasets: CIFAR-10 and ImageNet}
In this section, I give a brief explanation of CIFAR-10 and ImageNet datasets.

\subsection{CIFAR-10 dataset}
The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection
of images that are commonly used to train neural networks. It is one of the
most widely used datasets for machine learning research.

It consists of \textbf{60000} \bm{$32 \times 32$} colour images in
\textbf{10 classes}, with 6000 images per class. There are 50000 training
images and 10000 test images.

The dataset is divided into five training batches and one test batch, each with
10000 images. The test batch contains exactly 1000 randomly-selected images
from each class. The training batches contain the remaining images in random
order, but some training batches may contain more images from one class than
another. Between them, the training batches contain exactly 5000 images from
each class.

The 10 different classes represent airplanes, cars, birds, cats, deer, dogs,
frogs, horses, ships, and trucks (\autoref{fig:CIFAR_10}).

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/results/CIFAR_10.png}
    \centering
    \caption{CIFAR-10 sample images}\label{fig:CIFAR_10}
\end{figure}

The classes are completely mutually exclusive. There is no overlap between
automobiles and trucks. ``Automobile'' includes sedans, SUVs, things of that
sort. ``Truck'' includes only big trucks. Neither includes pick-up trucks.

\subsection{ImageNet 2012 dataset}
ImageNet is an image dataset organized according to the WordNet hierarchy. Each
meaningful concept in WordNet, possibly described by multiple words or word
phrases, is called a ``synonym set'' or ``synset''. There are more than 100000
synsets in WordNet, majority of them are nouns (80000+). In ImageNet, there are
on average 1000 images to illustrate each synset.
Images of each concept are quality-controlled and human-annotated. In its
completion, ImageNet will offer tens of millions of cleanly sorted images for
most of the concepts in the WordNet hierarchy (\autoref{fig:imagenet})

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/results/imagenet.png}
    \centering
    \caption{ImageNet sample images}\label{fig:imagenet}
\end{figure}

The dataset I've used for the experiments has:

\begin{itemize}
    \item Train split: \textbf{1281167 images}
    \item Validation split: \textbf{50000 images}
\end{itemize}

The ILSVRC uses a ``trimmed'' list of only \textbf{1000 image categories} or
``classes'', including 90 of the 120 dog breeds classified by the full ImageNet
schema.

\section{Experiment results}
After a brief explanation of the model and the datasets used for the
experiments, in this section I show what results I have in pruning MobileNet v1
with CIFAR-10 and ImageNet.

\subsection{Environments}
The environment used for running CIFAR-10 experiments has the following
characteristics:

\begin{itemize}
    \item \textbf{CPU and memory:} 2 x Intel Xeon Gold 5120T CPU @ 2.20GHz, 28
        cores (56 threads) total, 64GB memory
    \item \textbf{Graphic:} NVIDIA TITAN Xp, 12GB GDDR5X, 3840 cores, CUDA
        driver version: 460.27.04, CUDA version: 11.2
    \item \textbf{Operating System:} Ubuntu 18.04.5 LTS, kernel
        5.4.0\-60-generic
    \item \textbf{Software Stack:} Python 3.8.5, TensorFlow 2.4.0, TFMOT
        0.5.0.dev20210206 with my patch for heuristic distribution, and
        dependencies. Everything has been isolated using conda
        (\url{https://docs.conda.io/en/latest/})
\end{itemize}

The environment used for running ImageNet experiments has the following
characteristics:

\begin{itemize}
    \item \textbf{CPU and memory:} AWS p3.2xlarge, 8 vCPU, 61GB memory
    \item \textbf{Graphic:} NVIDIA Tesla V100, 16GB, 5120 cores, CUDA Driver
        Version: 450.80.02, CUDA Version: 11.0
    \item \textbf{Operating System:} Ubuntu 18.04.5 LTS, kernel
        5.4.0\-1037-aws
    \item \textbf{Software Stack:} Python 3.7.6, TensorFlow 2.4.1, TFMOT
        0.5.0.dev20210206 with my patch for heuristic distribution, and
        dependencies. Everything has been isolated using conda
        (\url{https://docs.conda.io/en/latest/})
\end{itemize}

The reason I chose AWS for ImageNet is because of the time the training takes
with this dataset: one epoch is about 1.5h and I needed to run 20 epochs.
As I show later, the number of experiments are 20 and AWS gives me the
flexibility to have multiple instances in parallel.

\subsection{Pipeline details}
In order to run experiments, I've developed a custom pipeline (not included in
this thesis) which executes the following steps:

\begin{enumerate}
    \item Load in memory the model architecture from a pre-trained checkpoint
    \item Convert the model into float32, int8 and int16 tflite format
    \item Prune the original model with given parameters
    \item Check model sparsity and calculate loss, TOP-1 and TOP-5 accuracies
    \item Convert the pruned model into float32, int8 and int16 tflite format
    \item For every tflite generated, run:
    \begin{enumerate}
        \item Accuracy evaluation on the validation dataset (10000 samples)
        \item Get the size and compressed size of the tflite file
        \item Only for int8/int16 tflite files: run Ethos-U vela
            (\autoref{sub:vela}) to get an estimation of the inference speed on
            Ethos-U NPU and the tflite size of the vela generated tflite file.
    \end{enumerate}
\end{enumerate}

The above pipeline has been run passing different sparsity level and
distributions. I ran 10 pipelines for every dataset: (Heuristic, 0.5),
(Uniform, 0.5), (Heuristic, 0.75), (Uniform, 0.75), (Heuristic, 0.8),
(Uniform, 0.8), (Heuristic, 0.85), (Uniform, 0.85), (Heuristic, 0.9),
(Uniform, 0.9).

The pruning scheduler chosen is \texttt{PolynomialDecay}: sparsity is
introduced slowly, model weights can take into account this impact and become
robust to the effect of weights being dropped.

Every run of the pipeline gives the following metrics:
\begin{enumerate}
    \item Keras model: loss, TOP-1 and TOP-5 accuracies
    \item For every tflite file: sizes of the original model, compressed, and
        pruned compressed
    \item For every int8/int16 tflite file: inference speed and vela compressed
        size
\end{enumerate}

Later I'll show graph that compares the above metrics across sparsity levels
and distributions.

\subsection{Ethos-U Vela}\label{sub:vela}
Vela is a tool used to compile a TensorFlow Lite for Microcontroller (tflite)
neural network model into an optimised version that can run on an embedded
system containing an Arm Ethos-U NPU\@.

In order to be accelerated by the Ethos-U NPU the network operators must be
quantised to either 8-bit (unsigned or signed) or 16-bit (signed).

The optimised model will contain TensorFlow Lite Custom operators for those
parts of the model that can be accelerated by the Ethos-U NPU\@. Parts of the
model that cannot be accelerated are left unchanged and will instead run on the
Cortex-M series CPU using an appropriate kernel (such as the Arm optimised
CMSIS-NN kernels).

After compilation the optimised model can only be run on an Ethos-U NPU
embedded system.

The tool will also generate performance estimates for the compiled model.

For more information about vela, please refer to PyPi home page
\url{https://pypi.org/project/ethos-u-vela/}

\subsubsection{Vela memory optimization}
The Vela compiler also performs various memory optimizations to reduce both the
permanent (for example flash) and runtime (for example SRAM) memory
requirements.
One such technique for permanent storage is the compression of all the weights
in the model.

Another technique is cascading, which addresses the runtime memory usage.
Cascading reduces the maximum memory requirement by splitting the feature maps
(FM) of a group of consecutively supported operators into stripes. A stripe can
be either the full or partial width of the FM\@. And it can be the full or
partial height of the FM\@. Each stripe in turn is then run through all the
operators in the group.

The parts of the model that can be optimized and accelerated are grouped and
converted into TensorFlow Lite custom operators. The operators are then
compiled into a command stream that can be executed by the Ethos-U NPU\@.

Finally, the optimized model is written out as a $TFL\mu$ model and a
Performance Estimation report is generated that provides statistics, such as
memory usage and inference time.

The compiler includes numerous configuration options that allow you to specify
various aspects of the embedded system configuration (for example the Ethos-U
NPU configuration, memory types, and memory sizes). There are also options to
control the types of optimization that are performed during the compilation
process.\cite{vela_compiler}

\subsection{Results for MobileNet v1 with CIFAR-10}
For running this experiment the classical MobileNet v1 architecture has been
slightly modified on three layers.

The original MobileNet architecture is designed for ImageNet dataset where
images' size is $224\times224\times3$. Because CIFAR-10 images have smaller
size ($32\times32$) I had to modify slightly the architecture.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/results/mobilenet_cifar10_diff.png}
    \centering
    \caption{MobileNet v1 changes for CIFAR-10}\label{fig:mobilenet_cifar10_diff}
\end{figure}

The main change is that in the first layers the \texttt{stride=2} has been
replaced with \texttt{stride=1}.
\autoref{fig:mobilenet_cifar10_diff} shows what the difference is between
MobileNet for CIFAR-10 and the classical architecture.

Experiment results: \todo{Experiment results with CIFAR-10}

\subsection{Results MobileNet v1 with ImageNet 2012}
After seen the results using CIFAR-10, in this section the dataset used is
ILSVRC (ImageNet Large Scale Visual Recognition Challenge) 2012, commonly known
as ImageNet.

Experiment results: \todo{Experiment results with ImageNet}
