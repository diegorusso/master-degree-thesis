\chapter{Introduction}
In the recent years we have been witnessing an incredible increase of
Artificial Intelligence (AI) applications and services.
Machine Learning (ML) and Deep Learning (DL) have been outperforming other
classical AI algorithms in many fields, like face recognition and natural
language processing.

This has been possible thanks to the explosion of data availability and compute
power, both needed and essential for ``training'' these neural networks.
Once the neural network has been trained, it can be used for the target that it
has been trained for: this phase is called ``inference''.
In \autoref{sec:training} I explain more in details these phases and what lies
in between.

Applications that requires some intelligence are computation intensive
demanding CPU, GPU, memory hence power out of the device and this makes the
application not always available to the end user.
In the last years the performance of these devices has been getting better and
better but still not sufficient for all more compute intensive applications.

The solution is to have a centralised data management and processing
(see~\ref{fig:centralised_intelligence}): the device collects data, send the
data to the cloud for processing and it receives back a response to their
request. This is called \textbf{centralised intelligence}.
Although this design is the basis of many applications, it presents some
drawbacks:
\begin{itemize}
    \item \textbf{amount of data}: the data generated by the device is sent to
        the cloud for processing. The amount of this data is not trivial and
        this could put under stress communication channels.
    \item \textbf{persistent connection}: in order to have a working
        application a persistent network connection between the device and the
        cloud computing is required.
    \item \textbf{real-time}: some applications cannot function properly when
        in presence of latency introduced by the communication between the
        device and cloud and any delay in processing the request by the cloud.
    \item \textbf{data privacy}: the device might send personal data to the
        cloud exposing privacy issues in case of hacking of leaking of
        information from the cloud provide.
\end{itemize}

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/introduction/centralised_intelligence.png}
    \centering
    \caption{Centralised Intelligence}\label{fig:centralised_intelligence}
\end{figure}

In order to overcome to the above issues, a different approach is needed where
the data processing is not centralised but distributed closer where it is
generated: this different approach is called \textbf{edge computing},
Computing, storage and networking resources are located at the edge of the
network (IoT gateways, routers, etc\ldots) whilst end devices like mobile
phones and IoT devices request services from edge servers are called edge
devices.
It is easy to understand how this approach can address some of the issues in
the centralised intelligence: low latency between edge devices and edge servers
and data exchange and data privacy are some how mitigated.
It is worth noticing that edge computing is not a replacement for cloud
computing. On the contrary, it complements cloud computing and both are
targeting different kind of applications.

If we push a little more the design, the basis of edge computing combined with
AI creates what is called
\textbf{edge or mobile intelligence} (see~\ref{fig:edge_intelligence}).

This means that the data collection, caching, processing and analysis happens
on the device where the data is generated.
With this model latency, data privacy, network load and communication are all
contained and improved, giving a better experience to the end user and making
the application more reliable.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/introduction/edge_intelligence.png}
    \centering
    \caption{Edge Intelligence}\label{fig:edge_intelligence}
\end{figure}

With more powerful end devices we start having edge intelligence applications
in our pockets like smart suggestions on the keyboards based on the context of
the text, photos application with integrated face recognition (note: no
personal data is sent to the cloud) based on contacts stored on the mobile and
voice recognition commands for offline translation and on mobile actions.
Other examples are also self-driving cars, real-time applications and medical
devices but also noise cancellation on video call application like Microsoft
Teams. Interestingly enough, as counter example Google Meet uses its model on
the cloud leveraging Google TPU infrastructure.

In this thesis I focus mainly on the \textbf{edge inference} (~\ref{fig:edge_inference})
and specifically how the models can be optimised in order to reduce memory
footprint, size and compression without losing any accuracy in the prediction.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/introduction/edge_inference.png}
    \centering
    \caption{Edge Inference}\label{fig:edge_inference}
\end{figure}

Edge inference is the final step of a model life cycle where the trained model
is used to infer new and unseen data via a forward pass of the neural network.
This step happens on the edge device and it presents some challenges due to the
limited amount of compute power and memory on the device itself.

In order to work correctly end efficiently the model needs to go through a
series of optimisations that decreases the power/memory consumption and latency
whilst maintaining the accuracy at acceptable levels \- ideally without
incurring in any decrease.

As the figure~\ref{fig:edge_inference} shows, there are different techniques
that it can be used to optimise models for edge inference. These are:
\begin{itemize}
    \item \textbf{Model design}: let the machines themselves design optimal
        models or human-invented architectures
    \item \textbf{Acceleration}: hardware acceleration mainly focus on parallel
        execution while software acceleration focuses on optimising resource
        management and compilers, based on compressed models
    \item \textbf{Compression}: low-rank approximation, knowledge distillation,
        compact layer design, network pruning and parameters quantisation are
        few techniques in order to achieve model compression
\end{itemize}

The above introduction~\cite{xu2020edge} sets the background for this thesis where
I focus on a specific technique of model pruning.
In \autoref{sec:training} I give an overview of the entire flow of an
intelligence application giving a brief explanation of every step of the flow.

In \autoref{sec:MO} I explain the main techniques for model optimisation for
deployment, explaining what they are, pros and cons.

In \autoref{ch:pruning} I show the pruning technique more in details and
the section \autoref{sec:heuristic} illustrate the theory behind the per-layer
pruning configuration with heuristic.

In \autoref{ch:implementation} I show the code I have implemented in
TensorFlow Model Optimisation giving full explanation and some working
examples.

Finally in \autoref{ch:results} I report experiment results on few well known
neural networks showing the benefits of this approach.

Unless specified otherwise, \textbf{all the examples, code and documentation
assume the use of TensorFlow (\url{https://www.tensorflow.org/}) and its
ecosystem.}

\section{From training to inference}\label{sec:training}

The pillars of edge intelligence are \textbf{data, model and computation}.
We need big amount of data in order to train a model that behaves as expected.
Computation is needed throughout the whole process, from training to edge
inference.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/introduction/training_inference_flow.png}
    \centering
    \caption{From training to edge inference}\label{fig:training_inference}
\end{figure}

The figure \autoref{fig:training_inference} shows a classic life cycle flow of
an intelligent application. A brief explanation of each step:

\begin{itemize}
    \item \textbf{Train a model}: once decided what the task is, the right
        model needs to be used. There are different options: create a custom
        model, use a pre-trained model or use Transfer Learning on a
        pre-trained model.
    \item \textbf{Convert the model}: once the model is trained, it needs to be
        converted in a special format (\texttt{.tflite}) that can be used on
        edge devices in efficient manner.
    \item \textbf{Optimise the model}: that's the key phase where the model is
        optimised to use less space/memory and to have a faster inference by
        decreasing the latency. More details will be presented in section
        \autoref{sec:MO}.
    \item \textbf{Deploy the model at Edge}: once the model has been optimised,
        it is ready to be deployed to the edge device.
    \item \textbf{Make inference at Edge}: this is the last step where finally
        the model is used to do inference on new data that it has never seen
        and hopefully giving expected results in a timely fashion.
\end{itemize}

The above flow shows the big picture of an intelligent application life
cycle~\cite{tflite:intro} and gives an idea of the complexity behind the
creation of an edge intelligent application.
In the rest of the chapter, I will explain what techniques are available in
TensorFlow in order to \textbf{optimise the model}.

\section{Model optimisations for deployment}\label{sec:MO}

To optimise TensorFlow models, the ecosystem offers \textit{TensorFlow Model
Optimisation Toolkit} (abbrev. \textit{TFMOT}) which minimizes the complexity
of optimizing machine learning inference.

Inference efficiency is a critical concern when deploying machine learning
models because of latency, memory utilization, and in many cases power
consumption. Particularly on edge devices, such as mobile and Internet of
Things (IoT), resources are further constrained, and model size and efficiency
of computation become a major concern.

Computational demand for training grows with the number of models trained on
different architectures, whereas the computational demand for inference grows
in proportion to the number of users.

Model optimisation is useful, among other things, for:

\begin{itemize}
    \item Reducing latency and cost for inference for both cloud and edge
        devices (e.g.\ mobile, IoT).
    \item Deploying models on edge devices with restrictions on processing,
        memory and/or power-consumption.
    \item Reducing payload size for over-the-air model updates.
    \item Enabling execution on hardware restricted-to or optimised-for
        fixed-point operations.
    \item Optimizing models for special purpose hardware accelerators.
\end{itemize}

The available techniques are:

\begin{itemize}
    \item \textbf{Clustering}: Clustered models are those where the original
        model's parameters are replaced with a smaller number of unique values.
    \item \textbf{Quantization}: Quantized models are those where we represent
        the models with lower precision, such as 8-bit integers as opposed to
        32-bit float. Lower precision is a requirement to leverage certain
        hardware.
    \item \textbf{Pruning}: Sparse models are those where connections in
        between operators (i.e.\ neural network layers) have been pruned,
        introducing zeros to the parameter tensors.
\end{itemize}

When pre-optimised models and post-training tools do not satisfy your use case,
the next step is to try the different training-time tools.

Training time tools piggyback on the model's loss function over the training
data such that the model can ``adapt'' to the changes brought by the
optimisation technique.~\cite{tfmot:intro}

\subsection{Clustering}
Clustering, or weight sharing, reduces the number of unique weight values in a
model, leading to benefits for deployment. It first groups the weights of each
layer into N clusters, then shares the cluster's centroid value for all the
weights belonging to the cluster.

This technique brings improvements via model compression. Future framework
support can unlock memory footprint improvements that can make a crucial
difference for deploying deep learning models on embedded systems with limited
resources.

According to Google experiments, models can be compressed up to 5x with minimal
loss of accuracy. Here below results on
vision~\autoref{fig:clustering_image_classification} and speech
models~\autoref{fig:clustering_keyword_spotting}.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/clustering_image_classification.png}
    \centering
    \caption{Clustering on Image classification}\label{fig:clustering_image_classification}
\end{figure}


\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/clustering_keyword_spotting.png}
    \centering
    \caption{Clustering on Keyword spotting}\label{fig:clustering_keyword_spotting}
\end{figure}

Size of compressed \texttt{.tflite} refers to the size of the zipped
\texttt{.tflite} file obtained from the model from the following process:

\begin{enumerate}
    \item Serialize the Keras model into \texttt{.h5} file
    \item Convert the \texttt{.h5} file into \texttt{.tflite}
    \item Compress the \texttt{.tflite} file into a zip
\end{enumerate}

The weight clustering implementation is based on the \textit{Deep Compression:
Compressing Deep Neural Networks With Pruning, Trained Quantization and Huffman
Coding}~\cite{han2015deep}~\cite{tfmot:clustering}

\subsection{Quantization}
~\cite{tfmot:quantization_post_training}
~\cite{tfmot:quantization_training}

\subsection{Pruning}
~\cite{tfmot:pruning}

\subsection{Combine multiple optimizations}