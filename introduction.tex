\chapter{Introduction}\label{ch:introduction}
In the recent years we have been witnessing an incredible increase of
Artificial Intelligence (AI) applications and services.
Machine Learning (ML) and Deep Learning (DL) have been outperforming other
classical AI algorithms in many fields, like face recognition and natural
language processing.

This has been possible thanks to an exceptional increase of data availability
and computational power, both critical for realizing the ``training'' phase of
neural networks.
Once the neural network has been trained, it can be used for the actual purpose
that it has been trained for: this phase is called ``inference'', and it
consists in providing the network with the input and computing the output
response, e.g., recognition, classification, regression, prediction,
etc\ldots

The training phase is the most computationally intensive, and usually it is
done off-line on special purpose powerful hardware resources where the amount
of ``big data'' required for training is available. On the other hand, the
\textit{inference} phase is held where the intelligence service is made
available, and the computational and memory requirements of this phase,
although less than those required for training are neither negligible nor
small.
In \autoref{sec:training} these phases, their characteristics and relationships
are explained more in details.

Applications that require to apply intelligent inference are therefore
computationally intensive and quite demanding CPUs, GPUs, and memory power out
of the device where the application is deployed and this does not always make
possible to deliver efficiently the application to edge devices, i.e., devices
close to the end user.
In the last years the performance of those edge devices has been getting better
and better but still not sufficient for all more computationally intensive
applications.

A common solution to overcome this limitation is to adopt a centralised data
management and processing approach
(see~\ref{fig:centralised_intelligence}): the device collects data, send the
data to the cloud for processing and it receives back a response to their
request. We refer to this approach as
\textbf{\textit{centralised intelligence}}.
Although this design is the basis of many applications, it presents some
drawbacks:
\begin{itemize}
    \item \textbf{amount of data}: the data generated by the device is sent to
        the cloud for processing. The amount of this data is not trivial, and
        this could overload communication channels.
    \item \textbf{persistent connection}: in order to ensure a robust and
        reliable application a persistent network connection between the device
        and the cloud computing is required.
    \item \textbf{real-time}: some applications cannot function properly when
        in presence of latency introduced by the communication between the
        device and cloud and any delay introduced by the cloud in processing
        the request.
    \item \textbf{data privacy}: the device might send personal data in transit
        to the cloud, arising privacy issues in case of hacking or accidental
        leaking of information, on the other hand cryptographic encoding of
        channels can introduce a relevant computational overhead.
\end{itemize}

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/introduction/centralised_intelligence.png}
    \centering
    \caption{Centralised intelligence}\label{fig:centralised_intelligence}
\end{figure}

In order to overcome to the above issues, a different approach is needed where
the data processing is not centralised but distributed closer where it is
generated: this different approach is called \textbf{\textit{edge computing}},
Computing, storage and networking resources are located at the edge of the
network (IoT gateways, routers, etc\ldots) whilst end devices like mobile
phones and IoT devices request services from edge servers are called edge
devices.
It is easy to understand how this approach can address some of the issues
arising in a centralised intelligence architecture: low latency between edge
devices and edge servers and data exchange and data privacy are somehow
mitigated.
It is worth noticing that edge computing is not a replacement for cloud
computing. On the contrary, it complements cloud computing, and both are
targeting different kind of applications.

If we extend a little more according to this design perspective, the basis of
edge computing combined with AI creates what is called
\textbf{edge or mobile intelligence} (see~\ref{fig:edge_intelligence}).

This means that the data collection, caching, processing and analysis can take
place locally, i.e., on the device where the data is generated.
With this architectural model: latency, data privacy, network and communication
load are all contained and improved, giving a better experience to the end user
and eventually making the final applications and the whole system architecture
more reliable.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/introduction/edge_intelligence.png}
    \centering
    \caption{Edge intelligence}\label{fig:edge_intelligence}
\end{figure}

Having more powerful end devices allows to bring edge intelligence applications
in end users ``pockets'', like smart suggestions on the keyboards based on the
context of the text, photos application with integrated face recognition, based
on contacts stored on the mobile and voice recognition commands for offline
translation, and it is worth to point out that in all these cases: no personal
data as sent or stored in the cloud, like it is generally happening now.
Other examples are also self-driving cars, real-time applications and medical
devices but also noise cancellation on video call application (interestingly
enough, as counter example, Google Meet uses its noise cancellation model on
the cloud leveraging on Google TPU infrastructure, acquiring sample of user
ambience sounds\cite{googlemeetnoise}).

This thesis focuses mainly on the \textbf{\textit{edge inference}}
(~\ref{fig:edge_inference}) and specifically investigated on how the models can
be optimised in order to reduce memory footprint, size and improve compression
without losing accuracy in the prediction.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/introduction/edge_inference.png}
    \centering
    \caption{Edge inference}\label{fig:edge_inference}
\end{figure}

Edge inference is the final step of a \textit{neural network model life cycle}
where the trained model is used to infer new conclusion/prediction via a
forward pass of unseen data through the neural network.
This step take place on the edge device and it presents some challenges due to
the limited amount of computational power and memory available on the device.

In order to work correctly and efficiently the model needs to go through a
series of optimisations that decreases the power/memory consumption and latency
whilst maintaining the accuracy at acceptable levels \- ideally minimizing or
without incurring in any loss.

As summarized in figure~\ref{fig:edge_inference} shows, there are different
techniques that they can be used to optimise models for edge inference. These
are:

\begin{itemize}
    \item \textbf{Model design}: let the machines themselves design optimal
        models or human-invented architectures~\cite{xu2020edge}.
    \item \textbf{Acceleration}: hardware acceleration mainly focuses on
        parallel execution while software acceleration focuses on optimising
        resource management and compilers, based on compressed
        models~\cite{xu2020edge}.
    \item \textbf{Compression}: low-rank approximation, knowledge distillation,
        compact layer design, network pruning and parameters quantization are
        few techniques in order to achieve model compression~\cite{xu2020edge}.
\end{itemize}

With this introduction~\cite{xu2020edge} we have set the background for this
thesis where I focus on a specific technique of model pruning.
In \autoref{sec:training} I give an overview of the entire flow of an
intelligence application giving a brief explanation of every step of the flow.

In \autoref{sec:MO} the main techniques of model optimisation for
deployment, explaining are described and pros and cons discussed.

In \autoref{ch:pruning} the pruning technique is shown more in details and
section \autoref{sec:heuristic} illustrates the theory behind the
\textit{per-layer pruning configuration with heuristic}.

In \autoref{ch:objective} I explain the per-layer pruning with heuristic and
the architecture of an implementation in TensorFlow Model Optimization giving
full explanation and a working example.

Finally in \autoref{ch:experiments} experimental results obtained experimenting
the pruning technique on well-known neural networks are presented and the
benefits of this approach are discussed.

Unless specified otherwise, \textbf{all the examples, code and documentation
assume the use of TensorFlow (\url{https://www.tensorflow.org/}) and its
ecosystem.}

\section{From training to inference}\label{sec:training}

The pillars of edge intelligence are \textbf{data, model and computation}.
In general, a big amount of \textit{data} is needed, in order to train a
\textit{model} that behaves as expected, while computational resources are
needed in different amount throughout the whole process, from training to edge
inference.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/introduction/training_inference_flow.png}
    \centering
    \caption{From training to edge inference}\label{fig:training_inference}
\end{figure}

The figure \autoref{fig:training_inference} shows a classic life cycle flow of
an intelligent application. A brief explanation of each step:

\begin{itemize}
    \item \textbf{Train a model}: once decided what the task is, the right
        model structure needs to be used. There are different options: create a
        custom model, use a pre-trained model or use Transfer Learning on a
        pre-trained model.
    \item \textbf{Convert the model}: once the model is trained, it needs to be
        converted in a special format (\texttt{.tflite}) that can be used on
        edge devices in efficient manner.
    \item \textbf{Optimise the model}: that's the key phase where the model is
        optimised to use less space/memory and to have a faster inference
        performance by decreasing the latency. More details will be presented
        in section \autoref{sec:MO}.
    \item \textbf{Deploy the model at Edge}: once the model has been optimised,
        it is ready to be deployed to the edge device.
    \item \textbf{Make inference at Edge}: this is the last step where finally
        the model is used to do inference on new data that it has never seen
        and hopefully providing expected results in a timely fashion.
\end{itemize}

The above flow shows the big picture of an \textit{intelligent application life
cycle}~\cite{tflite:intro} and gives an idea of the complexity behind the
creation of an edge intelligent application.
In the rest of the chapter, I will explain what techniques are available in
TensorFlow in order to \textbf{optimise the model}.

\section{Model optimisations for deployment}\label{sec:MO}

To optimise TensorFlow models, the ecosystem offers \textit{TensorFlow Model
Optimization Toolkit} (abbrev. \textit{TFMOT}) which minimizes the complexity
of optimising machine learning inference.

Inference efficiency is a critical concern when deploying machine learning
models because of latency, memory utilization, and in many cases power
consumption. Particularly on edge devices, such as mobile and Internet of
Things (IoT), resources are further constrained, and model size and efficiency
of computation become a major concern.

Model optimisation is useful, among other things, for:

\begin{itemize}
    \item Reducing latency and cost for inference for both cloud and edge
        devices (e.g., mobile, IoT).
        Latency in machine learning refers to the time taken by the model to
        process one unit of data when there is no parallelism i.e., time taken
        by MobileNet to classify an image.
    \item Deploying models on edge devices with restrictions on processing,
        memory and/or power-consumption.
    \item Reducing payload size for over-the-air model updates.
    \item Enabling execution on hardware restricted-to or optimised-for
        fixed-point operations.
    \item Optimising models for special purpose hardware accelerators.
\end{itemize}

The most common available techniques for model optimization are:

\begin{itemize}
    \item \textbf{Weight Clustering}: Clustered models are those where the
        original model's parameters are replaced with a smaller number of
        unique values~\cite{tfmot:intro}.
    \item \textbf{Quantization}: Quantized models are those where we represent
        the models with lower precision, such as 8-bit integers as opposed to
        32-bit float. Lower precision is a requirement to leverage certain
        hardware~\cite{tfmot:intro}.
    \item \textbf{Weight Pruning}: Sparse models are those where connections in
        between operators (i.e., neural network layers) have been pruned,
        introducing zeros to the parameter tensors~\cite{tfmot:intro}.
\end{itemize}

When pre-optimised models and post-training tools do not satisfy your use case,
the next step is to try the different training-time tools.

Training time tools piggyback on the model's loss function over the training
data such that the model can ``adapt'' to the changes brought by the
optimisation technique.~\cite{tfmot:intro}

\subsection{Weight clustering}
Clustering, or weight sharing, reduces the number of unique weight values in a
model, leading to benefits for deployment. It first groups the weights of each
layer into N clusters, then shares the cluster's centroid value for all the
weights belonging to the cluster.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/weight_clustering.png}
    \centering
    \caption{Weight clustering}\label{fig:weight_clustering}
\end{figure}

A brief explanation of \autoref{fig:weight_clustering}. For example, a layer in
the model contains a 4$\times$4 matrix of weights (represented by the
``weight matrix'' in \autoref{fig:weight_clustering}). Each weight is stored
using a float32 value. When the model is saved, 16 unique float32 values are
stored.

Weight clustering reduces the size of the model by replacing similar weights in
a layer with the same value. These values are found by running a clustering
algorithm over the model’s trained weights. The user can specify the number of
clusters (in this case, 4). This step is shown in ``Get centroids'' in the
diagram, and the 4 centroid values are shown in the ``Centroid'' table. Each
centroid value has an index (0\-3).

Next, each weight in the weight matrix is replaced with its centroid’s index.
This step is shown in ``Assign indices''. Now, instead of storing the original
weight matrix, the weight clustering algorithm can store the modified matrix
shown in ``Pull indices'' (containing the index of the centroid values), and
the centroid values themselves.

In this case, the size has been reduced from 16 unique floats, to 4 floats and
16 2-bit indices. The savings increase with larger matrix sizes.

Note that even if we still stored 16 floats, they now have just 4 distinct
values. Common compression tools (like zip) can now take advantage of the
redundancy in the data to achieve higher compression.

Weight clustering has an immediate advantage in reducing model storage and
transfer size across serialization formats, as a model with shared parameters
has a much higher compression rate than one without. This is like a sparse
(pruned) model, except that the compression benefit is achieved through
reducing the number of unique weights, while pruning achieves it through
setting weights below a certain threshold to zero. Once a model is clustered,
the benefit of the reduced size is available by passing it through any common
compression tool.

To further unlock the improvements in memory usage and speed at inference time
associated with clustering, specialized run-time or compiler software and
dedicated machine learning hardware is required.~\cite{tfmot:clustering_blog}

This technique brings shows improvements via model compression. Future
framework support can unlock memory footprint improvements that can make a
crucial difference for deploying deep learning models on embedded systems with
limited resources.

According to Google experiments~\cite{tfmot:clustering}, models can be
compressed up to 5x with minimal loss of accuracy. Here below results on
vision~\autoref{fig:clustering_image_classification} and speech
models~\autoref{fig:clustering_keyword_spotting}.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/clustering_image_classification.png}
    \centering
    \caption{Weight clustering on image classification}\label{fig:clustering_image_classification}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/clustering_keyword_spotting.png}
    \centering
    \caption{Weight clustering on keyword spotting}\label{fig:clustering_keyword_spotting}
\end{figure}

Size of compressed \texttt{.tflite} refers to the size of the zipped
\texttt{.tflite} file obtained from the model from the following process:

\begin{enumerate}
    \item Serialize the Keras model into \texttt{.h5} file
    \item Convert the \texttt{.h5} file into \texttt{.tflite}
    \item Compress the \texttt{.tflite} file into a zip
\end{enumerate}

The weight clustering implementation is based on the \textit{Deep Compression:
Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman
Coding}~\cite{han2015deep}~\cite{tfmot:clustering}

\subsection{Quantization}
There are two forms of quantization: \textbf{post-training quantization} and
\textbf{quantization aware training (QAT)}. The former is easy to use whilst
the latter often offers better model accuracy.

\subsubsection{Quantization is lossy}
Quantization is the process of transforming an ML model into an equivalent
representation that uses parameters and computations at a lower precision.
This improves the model's execution performance and efficiency

However, the process of going from higher to lower precision is intrinsically
lossy in nature. As seen in \autoref{fig:quantization}, quantization squeezes
a small range of floating-point values into a fixed number of information
buckets.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/quantization.png}
    \centering
    \caption{Quantization}\label{fig:quantization}
\end{figure}

This leads to information loss. The parameters (or weights) of a model can now
only take a small set of values and the minute differences between them are
lost. For example, all values in range [2.0, 2.3] may now be represented in one
single bucket. This is like rounding errors when fractional values are
represented as integers.

There are also other sources of loss. When these lossy numbers are used in
several multiply-add computations, these losses accumulate. Further, int8
values, which accumulate into int32 integers, need to be rescaled back to int8
values for the next computation, thus introducing more computational
error.~\cite{tfmot:quantization_blog}

\subsubsection{Quantization aware training}
Quantization aware training (QAT) emulates inference-time quantization,
creating a model that downstream tools will use to produce actually quantized
models. The quantized models use lower-precision (e.g., 8-bit instead of 32-bit
float), leading to benefits during deployment.

Quantization brings improvements via model compression and latency reduction.
With default API, Google has observed that the model shrinks by 4x and 1.5 \-
4x improvements in CPU latency. Eventually, latency improvements can be seen on
compatible machine learning accelerators (e.g., EdgeTPU and NNAPI).

At the time of writing, TensorFlow Model Optimization implementation of QAT is
still under development and has some limitations regarding to its functionality
(distributed training, limited support for Subclassed Models, RNN/LSTM models,
stable APIs, etc\ldots)

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/qat_image_classification.png}
    \centering
    \caption{QAT on image classification}\label{fig:qat_image_classification}
\end{figure}

The models in \autoref{fig:qat_image_classification} were tested on ImageNet
and evaluated in both TensorFlow and TFLite~\cite{tfmot:quantization_training}.

For a reference background, \textit{Quantization and Training
of Neural Networks for Efficient Integer-Arithmetic-Only Inference}
paper~\cite{Jacob_2018} introduces some concepts that this tool uses.
The TensorFlow Model Optimization implementation is not based on the same
ideas, and there are additional concepts used in this tool (e.g., per-axis
quantization)~\cite{tfmot:quantization_training}.

\subsubsection{Post-training Quantization}
Post-training quantization includes general techniques to reduce CPU and
hardware accelerator latency, processing, power, and model size with little
degradation in model accuracy. These techniques can be performed on an
already trained float TensorFlow model and applied during TensorFlow Lite
conversion. These techniques are enabled as options in the TensorFlow Lite
converter.

Two types of post-quantization exist:
\begin{itemize}
    \item \textbf{Quantizing weights}: Weights can be converted to types with
        reduced precision, such as 16-bit floats or 8-bit integers. Google
        generally recommends 16-bit floats for GPU acceleration and 8-bit
        integer for CPU execution. At inference, the most critically intensive
        parts are computed with 8-bits instead of floating point.
    \item \textbf{Full integer quantization of weights and activations}:
        Improve latency, processing, and power usage, and get access to
        integer-only hardware accelerators by making sure both weights and
        activations are quantized.  This requires a small representative data
        set. The resulting model will still take float input and output for
        convenience.
\end{itemize}

There are several post-training quantization options to choose from.
\autoref{fig:post_quantization_techniques} shows a summary table of the choices
and the benefits they provide.~\cite{tfmot:quantization_post_training}

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/post_quantization_techniques.png}
    \centering
    \caption{Post-quantization techniques}\label{fig:post_quantization_techniques}
\end{figure}

Compared to their float counterparts, quantized models are up to 2–4x faster on
CPU and 4x smaller (See \autoref{fig:post_quantization_latency}).

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/post_quantization_latency.png}
    \centering
    \caption{Post-quantization latency}\label{fig:post_quantization_latency}
\end{figure}

With just 100 calibration images from ImageNet dataset, fully quantized integer
models have comparable accuracy with their float versions (MobileNet v1 loses
1\%) (See \autoref{fig:post_quantization_accuracy}).

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/post_quantization_accuracy.png}
    \centering
    \caption{Post-quantization accuracy}\label{fig:post_quantization_accuracy}
\end{figure}

\subsection{Weight pruning}
Magnitude-based weight pruning gradually zeroes out model weights during the
training process to achieve model sparsity. Sparse models are easier to
compress, and we can skip the zeroes during inference for latency improvements.

This technique brings improvements via model compression. In the future,
framework support for this technique will provide latency improvements.
Google has seen up to 6x improvements in model compression with minimal loss of
accuracy.

The technique is being evaluated in various speech applications, such as speech
recognition and text-to-speech and has been experimented on across various
vision (see \autoref{fig:pruning_image_classification}) and translation
models (see \autoref{fig:pruning_translation}).~\cite{tfmot:pruning}

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/pruning_image_classification.png}
    \centering
    \caption{Weight pruning on image classification}\label{fig:pruning_image_classification}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/pruning_translation.png}
    \centering
    \caption{Weight pruning on translation}\label{fig:pruning_translation}
\end{figure}

In \autoref{ch:pruning} I will go more in details of this technique.

\subsection{Combine multiple optimisations}
The optimisation techniques shown in \autoref{sec:MO} bring excellent results
in terms of model size and latency.
The model can be optimised further combining those techniques. For instance,
the model can be optimised using pruning, post-training quantization and then
compress the result with any compression algorithm (e.g., zip, gzip)
Similarly the model can be optimised using clustering, post-training
quantization and compression.
The optimisation can be pushed further including all the previous techniques as
shown in \autoref{fig:collaborative_optimisation}:

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/collaborative_optimisation.png}
    \centering
    \caption{Collaborative optimisation}\label{fig:collaborative_optimisation}
\end{figure}

\begin{enumerate}
    \item \textbf{Weight Pruning}: remove all near-zero weights
    \item \textbf{Weight Clustering}: group all the remaining weights in
        different clusters
    \item \textbf{Quantization}: the weights representing the clusters are
        quantized from floating point at 32-bit to integer at 16/8bit
    \item \textbf{Compression}: the last step is to apply a compression
        algorithm to the model in order to improve deployment
\end{enumerate}

The paper \textit{Deep Compression: Compressing Deep Neural Networks with
Pruning, Trained Quantization and Huffman Coding}~\cite{han2015deep} shows an
optimisation pipeline (See \autoref{fig:combine_multiple_optimisations})

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/combine_multiple_optimisations.png}
    \centering
    \caption{Combine multiple optimisations}\label{fig:combine_multiple_optimisations}
\end{figure}

\autoref{fig:combine_multiple_optimisations} shows the three-stage compression
pipeline: pruning, quantization (with clustering) and Huffman coding. Pruning
reduces the number of weights by 10×, while quantization/clustering further
improves the compression rate: between 27× and 31×. Huffman coding gives more
compression: between 35× and 49×. The compression rate already included the
meta-data for sparse representation. The compression scheme doesn't incur any
accuracy loss.
