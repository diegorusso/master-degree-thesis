\chapter{Introduction}
In the recent years we have been witnessing an incredible increase of
Artificial Intelligence (AI) applications and services.
Machine Learning (ML) and Deep Learning (DL) have been outperforming other
classical AI algorithms in many fields, like face recognition and natural
language processing.

This has been possible thanks to the explosion of data availability and compute
power, both needed and essential for ``training'' these neural networks.
Once the neural network has been trained, it can be used for the target that it
has been trained for: this phase is called ``inference''.
In \autoref{sec:training} I explain more in details these phases and what lies
in between.

Applications that requires some intelligence are computation intensive
demanding CPU, GPU, memory hence power out of the device and this makes the
application not always available to the end user.
In the last years the performance of these devices has been getting better and
better but still not sufficient for all more compute intensive applications.

The solution is to have a centralised data management and processing
(see~\ref{fig:centralised_intelligence}): the device collects data, send the
data to the cloud for processing and it receives back a response to their
request. This is called \textbf{centralised intelligence}.
Although this design is the basis of many applications, it presents some
drawbacks:
\begin{itemize}
    \item \textbf{amount of data}: the data generated by the device is sent to
        the cloud for processing. The amount of this data is not trivial and
        this could put under stress communication channels.
    \item \textbf{persistent connection}: in order to have a working
        application a persistent network connection between the device and the
        cloud computing is required.
    \item \textbf{real-time}: some applications cannot function properly when
        in presence of latency introduced by the communication between the
        device and cloud and any delay in processing the request by the cloud.
    \item \textbf{data privacy}: the device might send personal data to the
        cloud exposing privacy issues in case of hacking of leaking of
        information from the cloud provide.
\end{itemize}

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/introduction/centralised_intelligence.png}
    \centering
    \caption{Centralised Intelligence}\label{fig:centralised_intelligence}
\end{figure}

In order to overcome to the above issues, a different approach is needed where
the data processing is not centralised but distributed closer where it is
generated: this different approach is called \textbf{edge computing},
Computing, storage and networking resources are located at the edge of the
network (IoT gateways, routers, etc\ldots) whilst end devices like mobile
phones and IoT devices request services from edge servers are called edge
devices.
It is easy to understand how this approach can address some of the issues in
the centralised intelligence: low latency between edge devices and edge servers
and data exchange and data privacy are some how mitigated.
It is worth noticing that edge computing is not a replacement for cloud
computing. On the contrary, it complements cloud computing and both are
targeting different kind of applications.

If we push a little more the design, the basis of edge computing combined with
AI creates what is called
\textbf{edge or mobile intelligence} (see~\ref{fig:edge_intelligence}).

This means that the data collection, caching, processing and analysis happens
on the device where the data is generated.
With this model latency, data privacy, network load and communication are all
contained and improved, giving a better experience to the end user and making
the application more reliable.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/introduction/edge_intelligence.png}
    \centering
    \caption{Edge Intelligence}\label{fig:edge_intelligence}
\end{figure}

With more powerful end devices we start having edge intelligence applications
in our pockets like smart suggestions on the keyboards based on the context of
the text, photos application with integrated face recognition (note: no
personal data is sent to the cloud) based on contacts stored on the mobile and
voice recognition commands for offline translation and on mobile actions.
Other examples are also self-driving cars, real-time applications and medical
devices but also noise cancellation on video call application like Microsoft
Teams. Interestingly enough, as counter example Google Meet uses its model on
the cloud leveraging Google TPU infrastructure.

In this thesis I focus mainly on the \textbf{edge inference} (~\ref{fig:edge_inference})
and specifically how the models can be optimised in order to reduce memory
footprint, size and compression without losing any accuracy in the prediction.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/introduction/edge_inference.png}
    \centering
    \caption{Edge Inference}\label{fig:edge_inference}
\end{figure}

Edge inference is the final step of a model life cycle where the trained model
is used to infer new and unseen data via a forward pass of the neural network.
This step happens on the edge device and it presents some challenges due to the
limited amount of compute power and memory on the device itself.

In order to work correctly end efficiently the model needs to go through a
series of optimisations that decreases the power/memory consumption and latency
whilst maintaining the accuracy at acceptable levels \- ideally without
incurring in any decrease.

As the figure~\ref{fig:edge_inference} shows, there are different techniques
that it can be used to optimise models for edge inference. These are:
\begin{itemize}
    \item \textbf{Model design}: let the machines themselves design optimal
        models or human-invented architectures
    \item \textbf{Acceleration}: hardware acceleration mainly focus on parallel
        execution while software acceleration focuses on optimising resource
        management and compilers, based on compressed models
    \item \textbf{Compression}: low-rank approximation, knowledge distillation,
        compact layer design, network pruning and parameters quantization are
        few techniques in order to achieve model compression
\end{itemize}

The above introduction~\cite{xu2020edge} sets the background for this thesis where
I focus on a specific technique of model pruning.
In \autoref{sec:training} I give an overview of the entire flow of an
intelligence application giving a brief explanation of every step of the flow.

In \autoref{sec:MO} I explain the main techniques for model optimisation for
deployment, explaining what they are, pros and cons.

In \autoref{ch:pruning} I show the pruning technique more in details and
the section \autoref{sec:heuristic} illustrate the theory behind the per-layer
pruning configuration with heuristic.

In \autoref{ch:implementation} I show the code I have implemented in
TensorFlow Model Optimisation giving full explanation and some working
examples.

Finally in \autoref{ch:results} I report experiment results on few well known
neural networks showing the benefits of this approach.

Unless specified otherwise, \textbf{all the examples, code and documentation
assume the use of TensorFlow (\url{https://www.tensorflow.org/}) and its
ecosystem.}

\section{From training to inference}\label{sec:training}

The pillars of edge intelligence are \textbf{data, model and computation}.
We need big amount of data in order to train a model that behaves as expected.
Computation is needed throughout the whole process, from training to edge
inference.

\begin{figure}[ht]
    \includegraphics[width=10cm]{images/introduction/training_inference_flow.png}
    \centering
    \caption{From training to edge inference}\label{fig:training_inference}
\end{figure}

The figure \autoref{fig:training_inference} shows a classic life cycle flow of
an intelligent application. A brief explanation of each step:

\begin{itemize}
    \item \textbf{Train a model}: once decided what the task is, the right
        model needs to be used. There are different options: create a custom
        model, use a pre-trained model or use Transfer Learning on a
        pre-trained model.
    \item \textbf{Convert the model}: once the model is trained, it needs to be
        converted in a special format (\texttt{.tflite}) that can be used on
        edge devices in efficient manner.
    \item \textbf{Optimise the model}: that's the key phase where the model is
        optimised to use less space/memory and to have a faster inference by
        decreasing the latency. More details will be presented in section
        \autoref{sec:MO}.
    \item \textbf{Deploy the model at Edge}: once the model has been optimised,
        it is ready to be deployed to the edge device.
    \item \textbf{Make inference at Edge}: this is the last step where finally
        the model is used to do inference on new data that it has never seen
        and hopefully giving expected results in a timely fashion.
\end{itemize}

The above flow shows the big picture of an intelligent application life
cycle~\cite{tflite:intro} and gives an idea of the complexity behind the
creation of an edge intelligent application.
In the rest of the chapter, I will explain what techniques are available in
TensorFlow in order to \textbf{optimise the model}.

\section{Model optimisations for deployment}\label{sec:MO}

To optimise TensorFlow models, the ecosystem offers \textit{TensorFlow Model
Optimisation Toolkit} (abbrev. \textit{TFMOT}) which minimizes the complexity
of optimizing machine learning inference.

Inference efficiency is a critical concern when deploying machine learning
models because of latency, memory utilization, and in many cases power
consumption. Particularly on edge devices, such as mobile and Internet of
Things (IoT), resources are further constrained, and model size and efficiency
of computation become a major concern.

Computational demand for training grows with the number of models trained on
different architectures, whereas the computational demand for inference grows
in proportion to the number of users.

Model optimisation is useful, among other things, for:

\begin{itemize}
    \item Reducing latency and cost for inference for both cloud and edge
        devices (e.g.\ mobile, IoT).
    \item Deploying models on edge devices with restrictions on processing,
        memory and/or power-consumption.
    \item Reducing payload size for over-the-air model updates.
    \item Enabling execution on hardware restricted-to or optimised-for
        fixed-point operations.
    \item Optimizing models for special purpose hardware accelerators.
\end{itemize}

The available techniques are:

\begin{itemize}
    \item \textbf{Weight Clustering}: Clustered models are those where the
        original model's parameters are replaced with a smaller number of
        unique values.
    \item \textbf{Quantization}: Quantized models are those where we represent
        the models with lower precision, such as 8-bit integers as opposed to
        32-bit float. Lower precision is a requirement to leverage certain
        hardware.
    \item \textbf{Weight Pruning}: Sparse models are those where connections in
        between operators (i.e.\ neural network layers) have been pruned,
        introducing zeros to the parameter tensors.
\end{itemize}

When pre-optimised models and post-training tools do not satisfy your use case,
the next step is to try the different training-time tools.

Training time tools piggyback on the model's loss function over the training
data such that the model can ``adapt'' to the changes brought by the
optimisation technique.~\cite{tfmot:intro}

\subsection{Weight Clustering}
Clustering, or weight sharing, reduces the number of unique weight values in a
model, leading to benefits for deployment. It first groups the weights of each
layer into N clusters, then shares the cluster's centroid value for all the
weights belonging to the cluster.

This technique brings improvements via model compression. Future framework
support can unlock memory footprint improvements that can make a crucial
difference for deploying deep learning models on embedded systems with limited
resources.

According to Google experiments, models can be compressed up to 5x with minimal
loss of accuracy. Here below results on
vision~\autoref{fig:clustering_image_classification} and speech
models~\autoref{fig:clustering_keyword_spotting}.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/clustering_image_classification.png}
    \centering
    \caption{Weight Clustering on Image Classification}\label{fig:clustering_image_classification}
\end{figure}


\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/clustering_keyword_spotting.png}
    \centering
    \caption{Weight Clustering on Keyword Spotting}\label{fig:clustering_keyword_spotting}
\end{figure}

Size of compressed \texttt{.tflite} refers to the size of the zipped
\texttt{.tflite} file obtained from the model from the following process:

\begin{enumerate}
    \item Serialize the Keras model into \texttt{.h5} file
    \item Convert the \texttt{.h5} file into \texttt{.tflite}
    \item Compress the \texttt{.tflite} file into a zip
\end{enumerate}

The weight clustering implementation is based on the \textit{Deep Compression:
Compressing Deep Neural Networks With Pruning, Trained Quantization and Huffman
Coding}~\cite{han2015deep}~\cite{tfmot:clustering}

\subsection{Quantization}
There are two forms of quantization: \textbf{post-training quantization} and
\textbf{quantization aware training (QAT)}. The former is easy to use whilst
the latter often offers better model accuracy.

\subsubsection{Quantization aware training}
Quantization aware training emulates inference-time quantization, creating a
model that downstream tools will use to produce actually quantized models. The
quantized models use lower-precision (e.g. 8-bit instead of 32-bit float),
leading to benefits during deployment.

Quantization brings improvements via model compression and latency reduction.
With default API, Google has observed that the model shrinks by 4x and 1.5 \-
4x improvements in CPU latency. Eventually, latency improvements can be seen on
compatible machine learning accelerators (e.g.: EdgeTPU and NNAPI).

At the time of writing, QAT is still under development and has some limitations
regarding to its functionality (distributed training, limited support for
Subclassed Models, RNN/LSTM models, stable APIs, etc\ldots)

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/qat_image_classification.png}
    \centering
    \caption{QAT on Image Classification}\label{fig:qat_image_classification}
\end{figure}

The models in \autoref{fig:qat_image_classification} were tested on Imagenet
and evaluated in both TensorFlow and TFLite.

For background on something similar, see the \textit{Quantization and Training
of Neural Networks for Efficient Integer-Arithmetic-Only Inference}
paper~\cite{Jacob_2018}.
This paper introduces some concepts that this tool uses.
The implementation is not exactly the same, and there are additional concepts
used in this tool (e.g.\ per-axis quantization).~\cite{tfmot:quantization_training}

\subsubsection{Post-training Quantization}
Post-training quantization includes general techniques to reduce CPU and
hardware accelerator latency, processing, power, and model size with little
degradation in model accuracy. These techniques can be performed on an
already-trained float TensorFlow model and applied during TensorFlow Lite
conversion. These techniques are enabled as options in the TensorFlow Lite
converter.

Two types of post-quantization exist:
\begin{itemize}
    \item \textbf{Quantizing weights}: Weights can be converted to types with
        reduced precision, such as 16 bit floats or 8 bit integers. Google
        generally recommends 16-bit floats for GPU acceleration and 8-bit
        integer for CPU execution. At inference, the most critically intensive
        parts are computed with 8 bits instead of floating point.
    \item \textbf{Full integer quantization of weights and activations}:
        Improve latency, processing, and power usage, and get access to
        integer-only hardware accelerators by making sure both weights and
        activations are quantized.  This requires a small representative data
        set. The resulting model will still take float input and output for
        convenience.
\end{itemize}

There are several post-training quantization options to choose from.
\autoref{fig:post_quantization_techniques} shows a summary table of the choices
and the benefits they provide.~\cite{tfmot:quantization_post_training}

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/post_quantization_techniques.png}
    \centering
    \caption{Post-quantization Techniques}\label{fig:post_quantization_techniques}
\end{figure}

\subsection{Weight Pruning}
Magnitude-based weight pruning gradually zeroes out model weights during the
training process to achieve model sparsity. Sparse models are easier to
compress, and we can skip the zeroes during inference for latency improvements.

This technique brings improvements via model compression. In the future,
framework support for this technique will provide latency improvements.
Google has seen up to 6x improvements in model compression with minimal loss of
accuracy.

The technique is being evaluated in various speech applications, such as speech
recognition and text-to-speech, and has been experimented on across various
visioni (see \autoref{fig:pruning_image_classification}) and translation
models (see \autoref{fig:pruning_translation}).~\cite{tfmot:pruning}

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/pruning_image_classification.png}
    \centering
    \caption{Weight Pruning on Image Classification}\label{fig:pruning_image_classification}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{images/introduction/pruning_translation.png}
    \centering
    \caption{Weight Pruning on Translation}\label{fig:pruning_translation}
\end{figure}

In \autoref{ch:pruning} I will go more in details of this technique.

\subsection{Combine multiple optimizations}
\lipsum[1]