\chapter{Objective: per-layer pruning with heuristic}\label{ch:objective}
This chapter regards the main objective of this thesis: the technique of
per-layer pruning with heuristic. It is divided in three main sections:
\autoref{sec:heuristic} gives the theoretical background, while section
\autoref{sec:architecturesolution} and \autoref{sec:mnistpipeline} focus on the
changes to be introduced in API in TensorFlow Model Optimization in order to
use this technique.

\paragraph{Arm Ltd.}
The work illustrated in this thesis is part of the ML Tooling project within
MLG (ML Group) at Arm Ltd.
The vision of the project is to advance open source tools for neural network
training and optimisation tools to enable partners and ecosystem to take
advantage of optimum performance and the full range of features available in
Arm hardware and software.

In order to maximize the performance and power advantages, NPUs (Neural Process
Unit\cite{ethos-u65,ethos-n78}) require the networks to be optimised and
converted to int8/int16 format.

Improving the functionality, coverage and user experience of existing ecosystem
tools will make it easy for developers to target ML for all Arm IP\@.
This project ensures that it is easy to use open source tools to prepare
machine learning models for running efficiently on Arm IP\@.
This includes approaches to quantizing and retraining networks to
8/16 bit, compressing weights, and reducing operations with pruning and
clustering trained weights.

\section{Per-layer pruning with heuristic}\label{sec:heuristic}
In \autoref{ch:pruning} various aspects are explained about pruning and how
TFMOT implements it. This is a needed prerequisites to fully comprehend the
content of this section.

\subsection{Distribution of sparsity}
TFMOT supports two pruning schedules: constant sparsity and polynomial decay.
The schedule specifies when and what sparsity to apply to the network and this
is applied equally across all layer of the network model.

This hints to the first distribution type a network model can have,
\textbf{uniform distribution}. With this distribution basically the same
pruning level is applied to all prunable layers, regardless of their operation
types, sizes, and shapes.
The uniform distribution is the only distribution implemented in TFMOT at the
time of writing.

The distribution can be based on \textbf{reinforcement learning} or
\textbf{optimization based}. The tuning of the sparsity distribution and other
hyper-parameters can be viewed as a search problem whose solutions can be
learned\cite{He_2018} by training a reinforcement learning agent or optimized
by techniques such as simulated annealing. This approach penalizes accuracy
loss and encourages model compression with single or multiple objectives.
Real or estimated hardware performance statistics can be used by this approach
to guide the tuning and produce target-specific optimized models.

Another option is to base the sparsity distribution on a \textbf{heuristic}.
The heuristic is any approach to problem solving or self-discovery that employs
a practical method that is not guaranteed to be optimal, perfect, or rational,
but is nevertheless sufficient for reaching an immediate, short-term goal or
approximation.
Where finding an optimal solution is impossible or impractical, heuristic
methods can be used to speed up the process of finding a satisfactory solution.
In short, heuristics can be considered as mental shortcuts that ease the
cognitive load of making a decision, without affecting performance in most
practical cases.\cite{heuristic}

\subsection{Heuristic details}
After seeing examples of sparsity distribution, in this subsection I explain
how the heuristic can affects the sparsity distribution.

The \textbf{\textit{target pruning ratio}} (\texttt{pr}) can be distributed
based on a simple heuristic: the more parameters a layer has, the more prunable
it becomes due to the redundancy.

The function uses a heuristic to calculate the optimum sparsity per each
individual layer while still maintaining the requested target pruning ratio.

The heuristic is based on the simple intuition that a layer with more weights
has more redundancy and, therefore, can have a higher pruning ratio without
affecting performance, while a lower number of prunable weights would be more
sensitive to pruning and therefore be assigned a smaller target pruning ratio.

More specifically, the per-layer target pruning ratio can be defined as

\begin{equation}
    pr_i = \alpha \cdot \log|layer_i|
\end{equation}

where $|layer_i|$ denotes the \textbf{number of parameters in $i^{th}$ layer},
and the \textbf{weighted coefficient $\alpha$} as

\begin{equation}
    \alpha = \frac{pr \cdot \sum|layer_i|}{\sum(|layer_i| \cdot \log|layer_i|)}
\end{equation}

This heuristic has been verified with many networks and shown its effectiveness
at distributing the overall target sparsity\cite{wu2018pocketflow}. I'll be
showing the effect of the heuristic in \autoref{ch:experiments}.

\subsubsection{Custom heuristic formula}
The above heuristic formula is a fixed one and the user doesn't have any
control of it. A useful extension would be to allow the user to specify his/her
own heuristic formula to define the sparsity distribution across layers of the
entire model.

In \autoref{sec:architecturesolution} I will show how the user can use a custom
formula to prune the entire model. For this to work, the object that implements
the formula needs to have access to the whole model.

If the user uses the default formula, it's already proven the final sparsity is
respected. This might not be the case when using custom formulas.
A sanity check mechanism is needed in order to ensure the custom formula
respects the final sparsity. If this is not the case, the user should be warned
that the custom formula might return unexpected results as it might produce a
sparsity different from the final one specified.

\subsection{What should it be pruned?}
Currently in TensorFlow it is possible to prune the following objects:
\begin{itemize}
    \item \textbf{Model}: this could be either sequential or functional.
    \item \textbf{List of layers}: it is implemented but not so supported or
        documented.
    \item \textbf{Layer}: this is the basic object that can be pruned with
        TensorFlow pruning API\@.
\end{itemize}

In order to prune using the heuristic, the algorithm need knowledge of the
whole model because it needs to apply different sparsity at layer level whilst
maintaining a specific target/final sparsity set by the user.

For this reason pruning with heuristic is \textbf{supported only on the whole
model} hence the single layer is not supported at all and the framework should
raise an error.

Pruning with heuristic to a list of layer doesn't have much sense and it
doesn't seem to be a use case for that hence it is not supported and the
framework should raise an error.

\subsection{Heuristic with PolynomialDecay scheduler}
The PolynomialDecay scheduler requires some special consideration when used
with heuristic.

\begin{lstlisting}[language=Python, label={lst:polynomialdecay},
    caption=PolynomialDecay Scheduler in TFMOT]
|*\ldots*|

pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.3,
    final_sparsity=0.8,
    begin_step=2000,
    end_step=4000)

|*\ldots*|
\end{lstlisting}

As it is shown in \autoref{lst:polynomialdecay}, the scheduler has both initial
and final sparsity.
There is an issue when the \textbf{initial sparsity is bigger than the final
sparsity} calculated by the heuristic for a specific layer.
For instance, if the heuristic calculates the sparsity of a layer to be 0.2,
this will clash with the \texttt{initial\_sparsity=0.3}.
In order to address this issue, a couple of approaches can be taken:

\begin{enumerate}
    \item When the initial sparsity is bigger than the layer's final sparsity
        calculated by the heuristic, the initial sparsity is ignored and set to
        zero. In this way, the final sparsity of the layer it will be always
        bigger than the initial sparsity. A warning is raised when this
        happens.
    \item When the initial sparsity is bigger than the layer's final sparsity
        calculated by the heuristic, the initial sparsity is considered in
        proportion of the final sparsity. In \autoref{lst:polynomialdecay} the
        \texttt{initial\_sparsity} is set to 0.3. If the heuristic calculates
        the final sparsity of a layer to be 0.2, then the initial sparsity for
        that layer will be set to $0.3 * 0.2 = 0.06$
        A warning is raised when this happens.
\end{enumerate}

The approach used in \autoref{sec:architecturesolution} is the first one: when
the initial sparsity is bigger than the sparsity that the heuristic has
calculated, the initial sparsity will be ignored and set to zero.
The reason why I implement the first one is to have some consistency in the
pruning API\@: in the second approach the \texttt{initial\_sparsity} has a
different semantic meaning at run time depending of the value of the layer's
final sparsity and this could be confusing for the user.

\section{Architecture of the solution}\label{sec:architecturesolution}
The whole implementation is contained in the TensorFlow Model Optimization
(TFMOT)
repository\footnote{\url{https://github.com/tensorflow/model-optimization}}
This because the pruning with heuristic is a training-time optimization and it
is an improvements of the current pruning API\@.

At the time of writing the code is not public yet: the upstream process usually
lasts for months and it involves various discussion with Google.

If the heuristic based pruning needs to be applied, few lines of the above
example need to be changed.

\begin{lstlisting}[language=Python, caption=Pruning with Heuristic]
|*\ldots*|

from tensorflow_model_optimization.python.core.api.experimental import (
    sparsity_distribution,
)

|*\ldots*|

pruning_params = {
  "pruning_schedule":
    tfmot.sparsity.keras.PolynomialDecay(
      initial_sparsity=0.4,
      final_sparsity=0.8,
      begin_step=0,
      end_step=end_step
    ),
  "sparsity_distribution":
    sparsity_distribution.HeuristicSparsityDistribution(),
}

|*\ldots*|

model_for_pruning = \
  sparsity_distribution.prune_low_magnitude_custom_distribution(
    sequential_model, **pruning_params
  )

|*\ldots*|
\end{lstlisting}

The main changes compared to classical API are:

\begin{itemize}
    \item Import the new experimental module (lines 3--5)
    \item Define the heuristic based sparsity distribution in the pruning
        parameters (lines 17--18)
    \item \texttt{prune\_low\_magnitude\_custom\_distribution} is called in
        favour of the classic method (\texttt{prune\_low\_magnitude})
\end{itemize}

There are few key points to highlight in the implementation:

\begin{itemize}
    \item \texttt{HeuristicSparsityDistribution} is a subclass of \linebreak
        \texttt{SparsityDistribution}: the subclass needs to implement
        \linebreak
        \texttt{sparsity\_function\string(self, model, target\_pruning\_ratio\string)}
        method in order to define the custom distribution to apply to model's
        layers.

        Actual layer's information are stored in a data structure inside the
        instance of the class and it will be exported using
        \texttt{get\_sparsity\_map} method.

        The class has also a mechanism that checks if the projected sparsity
        distribution honours the target pruning ration set by the user: in case
        it doesn't, it prints a warning message.
    \item \texttt{prune\_low\_magnitude\_custom\_distribution} is a wrapper
        around the classic \texttt{prune\_low\_magnitude} function. It has a
        similar signature and takes an additional parameter:
        \texttt{sparsity\_distribution} which should be an instance of
        \texttt{SparsityDistribution}.

        The \texttt{sparsity\_distribution} create the \texttt{sparsity\_map}
        which will be used to wrap model's layer with sparsity distribution
        information.

        The function will revert back to the classic
        \texttt{prune\_low\_magnitude} in the following cases:
        \texttt{sparsity\_distribution} is not an instance of \linebreak
        \texttt{SparsityDistribution} or it is not defined and the object to
        prune is not a sequential of functional model. In this cases a warning
        will be printed saying the heuristic cannot be applied.
\end{itemize}

For a deeper understanding of the implementation,
\autoref{appendix:distribution.py} shows the code of
\texttt{HeuristicSparsityDistribution} and \texttt{SparsityDistribution},
whilst \autoref{appendix:prune.py} shows
\texttt{prune\_low\_magnitude\_custom\_distribution}

\section{MNIST pipeline with heuristic pruning}\label{sec:mnistpipeline}
The implementation code of the MNIST pipeline with heuristic pruning can be
found in \autoref{appendix:mnist}.

At the very high level the pipeline does the following:
\begin{enumerate}
    \item Build, train and evaluate the model
    \item Save the model in a file
    \item Prune and evaluate the new model
    \item Save the pruned model in a file
    \item Show the models' sizes to show the effect of pruning
\end{enumerate}

The \autoref{lst:mnistpipelineoutput} shows the output of the execution of the
pipeline script. Just after the output I will be highlighting the key points to
observe.

\begin{lstlisting}[label={lst:mnistpipelineoutput},
    caption=MNIST pipeline output execution]
|*\textdollar*| python full_heuristic_pruning_mnist.py
train_images shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              (None, 28, 28, 32)        832
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 14, 14, 64)        51264
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0
_________________________________________________________________
dense (Dense)                (None, 1024)              3212288
_________________________________________________________________
dropout (Dropout)            (None, 1024)              0
_________________________________________________________________
dense_1 (Dense)              (None, 10)                10250
=================================================================
Total params: 3,274,634
Trainable params: 3,274,634
Non-trainable params: 0
_________________________________________________________________
Epoch 1/4
1688/1688 [==============================] - 8s 5ms/step - loss: 0.1155 - accuracy: 0.9641 - val_loss: 0.0413 - val_accuracy: 0.9873
Epoch 2/4
1688/1688 [==============================] - 8s 5ms/step - loss: 0.0398 - accuracy: 0.9875 - val_loss: 0.0354 - val_accuracy: 0.9898
Epoch 3/4
1688/1688 [==============================] - 8s 5ms/step - loss: 0.0285 - accuracy: 0.9910 - val_loss: 0.0386 - val_accuracy: 0.9888
Epoch 4/4
1688/1688 [==============================] - 8s 5ms/step - loss: 0.0215 - accuracy: 0.9933 - val_loss: 0.0363 - val_accuracy: 0.9912
Test loss: 0.026622682809829712
Test accuracy: 0.991599977016449
Saved model to: /tmp/mnist.h5
Layer "conv2d": 800 weights of which about 287 will be zeroed. Sparsity will be 0.3589671368157816.
Layer "conv2d_1": 51200 weights of which about 29814 will be zeroed. Sparsity will be 0.5823013278812128.
Layer "dense": 3211264 weights of which about 2583624 will be zeroed. Sparsity will be 0.8045506230249138.
Layer "dense_1": 10240 weights of which about 5078 will be zeroed. Sparsity will be 0.49587367241725167.
Total weights for the model: 3273504
Projected zero weights for the model: 2618803
Projected sparsity: 0.8

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
prune_low_magnitude_conv2d ( (None, 28, 28, 32)        1634
_________________________________________________________________
prune_low_magnitude_max_pool (None, 14, 14, 32)        1
_________________________________________________________________
prune_low_magnitude_conv2d_1 (None, 14, 14, 64)        102466
_________________________________________________________________
prune_low_magnitude_max_pool (None, 7, 7, 64)          1
_________________________________________________________________
prune_low_magnitude_flatten  (None, 3136)              1
_________________________________________________________________
prune_low_magnitude_dense (P (None, 1024)              6423554
_________________________________________________________________
prune_low_magnitude_dropout  (None, 1024)              1
_________________________________________________________________
prune_low_magnitude_dense_1  (None, 10)                20492
=================================================================
Total params: 6,548,150
Trainable params: 3,274,634
Non-trainable params: 3,273,516
_________________________________________________________________
Epoch 1/2
211/211 [==============================] - 3s 15ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0285 - val_accuracy: 0.9932
Epoch 2/2
211/211 [==============================] - 3s 15ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0313 - val_accuracy: 0.9923
Per layer sparsity:
conv2d :  800  weights,  0.35875  sparsity
conv2d_1 :  51200  weights,  0.5823046875  sparsity
dense :  3211264  weights,  0.804550482302296  sparsity
dense_1 :  10240  weights,  0.49589843749999996  sparsity
Sparse weights: 2618803.0
All weights: 3273504
Overall model sparsity :  0.7999999389033892
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              (None, 28, 28, 32)        832
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 14, 14, 64)        51264
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0
_________________________________________________________________
dense (Dense)                (None, 1024)              3212288
_________________________________________________________________
dropout (Dropout)            (None, 1024)              0
_________________________________________________________________
dense_1 (Dense)              (None, 10)                10250
=================================================================
Total params: 3,274,634
Trainable params: 3,274,634
Non-trainable params: 0
_________________________________________________________________
Test loss: 0.019717451184988022
Test accuracy: 0.9940000176429749
Saved model to: /tmp/pruned_mnist.h5
Size of /tmp/mnist.h5 is 13121.672 KB
Size of zipped /tmp/mnist.h5 is 12177.406 KB
Size of /tmp/pruned_mnist.h5 is 13121.672 KB
Size of zipped /tmp/pruned_mnist.h5 is 3880.847 KB
\end{lstlisting}

The output can be divided in three main blocks. These are:

\subsubsection{Lines 1--39}
The table with the list of layers is produced by the
\texttt{summary\string(\string)} method. The main thing to notice is the amount
of trainable and non trainable parameters: the latter are zero.

After the table there is the output produced by the
\texttt{fit\string(\string)} method follow by the loss and accuracy of the
model. The accuracy is \textbf{0.9915999} which is pretty good.

\subsubsection{Lines 40--109}
The output in this block belongs to pruning. For every prunable layer, the
projected sparsity is printed along with the number of weights that will be
zeroed. The total projected sparsity is the reported. These figures are
calculated by the \texttt{HeuristicSparsityDistribution} instance. At this
point the model is not pruned yet.

The summary table is printed again and the thing to notice is that
\textbf{Non-trainable} parameters are about the same amount of the trainable
ones: this is the effect of the layer augmentation by the \linebreak
\texttt{prune\_low\_magnitude\_custom\_distribution} function.
Every trainable parameter has a linked non-trainable parameter which represents
the sparsity mask: before the actual pruning all values of the non-trainable
parameters are set to 1.

After the pruning which runs for 2 epochs, the \textbf{actual sparsity} of the
model is calculated: the sparsity matches the projected sparsity printed in the
previous block. After the pruning, the model doesn't need to carry the extra
information hence it is stripped: in fact the table now shows the ``right''
amount of parameters. The stripping of these parameters if critical if the
model needs to be compressed.

The accuracy of the pruned model is \textbf{0.994} which is slightly better
than the original model.

\subsubsection{Lines 110--114}
Finally the last block of output shows information about sizes of the models
saved to disk. There are four models saved:
\begin{itemize}
    \item \textbf{mnist.h5} is 13121.672 KB
    \item \textbf{zipped mnist.h5} is 12177.406 KB
    \item \textbf{pruned\_mnist.h5} is 13121.672 KB
    \item \textbf{zipped pruned\_mnist.h5} is \textbf{3880.847 KB}
\end{itemize}
The non-zipped models have the same size: this is normal because the pruned
model, although it has 80\% of weights to zero, it stills needs variable
allocation to host these zeroed weights.

In the zipped models instead there is a big difference: the non-pruned models
is about \textbf{12MB} whilst the pruned one is \textbf{3.8MB}: this because
the pruned model has sparse weights and the one with zero can be easily
compressed reducing the size.

Considering the accuracy as well, it is incredible to see how the pruning
didn't impact it at all: a model with 80\% sparsity results with a
\textbf{slightly better accuracy} and about \textbf{30\% the size of the
original model.}
